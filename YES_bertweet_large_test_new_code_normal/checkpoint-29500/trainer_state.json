{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.9273425755804245,
  "eval_steps": 500,
  "global_step": 29500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.016702856188408218,
      "grad_norm": 8.390623092651367,
      "learning_rate": 1.9933188575246368e-05,
      "loss": 1.44,
      "step": 100
    },
    {
      "epoch": 0.033405712376816436,
      "grad_norm": 4.451144218444824,
      "learning_rate": 1.9866377150492734e-05,
      "loss": 1.394,
      "step": 200
    },
    {
      "epoch": 0.05010856856522465,
      "grad_norm": 6.343800067901611,
      "learning_rate": 1.9799565725739103e-05,
      "loss": 1.3726,
      "step": 300
    },
    {
      "epoch": 0.06681142475363287,
      "grad_norm": 4.964259624481201,
      "learning_rate": 1.973275430098547e-05,
      "loss": 1.4151,
      "step": 400
    },
    {
      "epoch": 0.0835142809420411,
      "grad_norm": 4.436954021453857,
      "learning_rate": 1.966594287623184e-05,
      "loss": 1.3968,
      "step": 500
    },
    {
      "epoch": 0.1002171371304493,
      "grad_norm": 4.0742268562316895,
      "learning_rate": 1.9599131451478205e-05,
      "loss": 1.397,
      "step": 600
    },
    {
      "epoch": 0.11691999331885752,
      "grad_norm": 11.877642631530762,
      "learning_rate": 1.953232002672457e-05,
      "loss": 1.438,
      "step": 700
    },
    {
      "epoch": 0.13362284950726575,
      "grad_norm": 3.8749890327453613,
      "learning_rate": 1.946550860197094e-05,
      "loss": 1.4161,
      "step": 800
    },
    {
      "epoch": 0.15032570569567397,
      "grad_norm": 8.51753044128418,
      "learning_rate": 1.9398697177217303e-05,
      "loss": 1.3965,
      "step": 900
    },
    {
      "epoch": 0.1670285618840822,
      "grad_norm": 4.022190570831299,
      "learning_rate": 1.9331885752463673e-05,
      "loss": 1.3408,
      "step": 1000
    },
    {
      "epoch": 0.18373141807249038,
      "grad_norm": 8.19897174835205,
      "learning_rate": 1.926507432771004e-05,
      "loss": 1.3675,
      "step": 1100
    },
    {
      "epoch": 0.2004342742608986,
      "grad_norm": 2.837575912475586,
      "learning_rate": 1.9198262902956405e-05,
      "loss": 1.4097,
      "step": 1200
    },
    {
      "epoch": 0.21713713044930683,
      "grad_norm": 3.619563341140747,
      "learning_rate": 1.9131451478202775e-05,
      "loss": 1.3813,
      "step": 1300
    },
    {
      "epoch": 0.23383998663771505,
      "grad_norm": 4.588515281677246,
      "learning_rate": 1.906464005344914e-05,
      "loss": 1.4222,
      "step": 1400
    },
    {
      "epoch": 0.25054284282612327,
      "grad_norm": 5.304233551025391,
      "learning_rate": 1.899782862869551e-05,
      "loss": 1.3907,
      "step": 1500
    },
    {
      "epoch": 0.2672456990145315,
      "grad_norm": 9.991127014160156,
      "learning_rate": 1.8931017203941876e-05,
      "loss": 1.3998,
      "step": 1600
    },
    {
      "epoch": 0.2839485552029397,
      "grad_norm": 4.8654022216796875,
      "learning_rate": 1.8864205779188243e-05,
      "loss": 1.3956,
      "step": 1700
    },
    {
      "epoch": 0.30065141139134793,
      "grad_norm": 4.241886615753174,
      "learning_rate": 1.8797394354434612e-05,
      "loss": 1.3626,
      "step": 1800
    },
    {
      "epoch": 0.31735426757975616,
      "grad_norm": 4.381624698638916,
      "learning_rate": 1.8730582929680975e-05,
      "loss": 1.4078,
      "step": 1900
    },
    {
      "epoch": 0.3340571237681644,
      "grad_norm": 7.285508632659912,
      "learning_rate": 1.8663771504927344e-05,
      "loss": 1.3981,
      "step": 2000
    },
    {
      "epoch": 0.3507599799565726,
      "grad_norm": 6.013787269592285,
      "learning_rate": 1.859696008017371e-05,
      "loss": 1.4088,
      "step": 2100
    },
    {
      "epoch": 0.36746283614498076,
      "grad_norm": 3.928511142730713,
      "learning_rate": 1.853014865542008e-05,
      "loss": 1.3795,
      "step": 2200
    },
    {
      "epoch": 0.384165692333389,
      "grad_norm": 4.133041858673096,
      "learning_rate": 1.8463337230666446e-05,
      "loss": 1.4392,
      "step": 2300
    },
    {
      "epoch": 0.4008685485217972,
      "grad_norm": 5.785118103027344,
      "learning_rate": 1.8396525805912812e-05,
      "loss": 1.3786,
      "step": 2400
    },
    {
      "epoch": 0.41757140471020543,
      "grad_norm": 5.604117393493652,
      "learning_rate": 1.832971438115918e-05,
      "loss": 1.3851,
      "step": 2500
    },
    {
      "epoch": 0.43427426089861365,
      "grad_norm": 4.121088981628418,
      "learning_rate": 1.8262902956405544e-05,
      "loss": 1.3824,
      "step": 2600
    },
    {
      "epoch": 0.4509771170870219,
      "grad_norm": 6.4693684577941895,
      "learning_rate": 1.8196091531651914e-05,
      "loss": 1.4211,
      "step": 2700
    },
    {
      "epoch": 0.4676799732754301,
      "grad_norm": 5.836793899536133,
      "learning_rate": 1.812928010689828e-05,
      "loss": 1.4399,
      "step": 2800
    },
    {
      "epoch": 0.4843828294638383,
      "grad_norm": 4.581701278686523,
      "learning_rate": 1.8062468682144646e-05,
      "loss": 1.3707,
      "step": 2900
    },
    {
      "epoch": 0.5010856856522465,
      "grad_norm": 3.9898319244384766,
      "learning_rate": 1.7995657257391016e-05,
      "loss": 1.3922,
      "step": 3000
    },
    {
      "epoch": 0.5177885418406547,
      "grad_norm": 7.013339519500732,
      "learning_rate": 1.7928845832637382e-05,
      "loss": 1.4426,
      "step": 3100
    },
    {
      "epoch": 0.534491398029063,
      "grad_norm": 4.195032119750977,
      "learning_rate": 1.786203440788375e-05,
      "loss": 1.3647,
      "step": 3200
    },
    {
      "epoch": 0.5511942542174711,
      "grad_norm": 5.1717071533203125,
      "learning_rate": 1.7795222983130117e-05,
      "loss": 1.4034,
      "step": 3300
    },
    {
      "epoch": 0.5678971104058794,
      "grad_norm": 6.442337512969971,
      "learning_rate": 1.7728411558376483e-05,
      "loss": 1.4109,
      "step": 3400
    },
    {
      "epoch": 0.5845999665942876,
      "grad_norm": 3.985067367553711,
      "learning_rate": 1.7661600133622853e-05,
      "loss": 1.3949,
      "step": 3500
    },
    {
      "epoch": 0.6013028227826959,
      "grad_norm": 4.9820556640625,
      "learning_rate": 1.7594788708869216e-05,
      "loss": 1.4163,
      "step": 3600
    },
    {
      "epoch": 0.618005678971104,
      "grad_norm": 7.133368492126465,
      "learning_rate": 1.7527977284115585e-05,
      "loss": 1.3835,
      "step": 3700
    },
    {
      "epoch": 0.6347085351595123,
      "grad_norm": 4.983792781829834,
      "learning_rate": 1.746116585936195e-05,
      "loss": 1.3881,
      "step": 3800
    },
    {
      "epoch": 0.6514113913479205,
      "grad_norm": 4.64850378036499,
      "learning_rate": 1.739435443460832e-05,
      "loss": 1.4342,
      "step": 3900
    },
    {
      "epoch": 0.6681142475363288,
      "grad_norm": 6.126297950744629,
      "learning_rate": 1.7327543009854687e-05,
      "loss": 1.4026,
      "step": 4000
    },
    {
      "epoch": 0.6848171037247369,
      "grad_norm": 5.378951072692871,
      "learning_rate": 1.7260731585101053e-05,
      "loss": 1.409,
      "step": 4100
    },
    {
      "epoch": 0.7015199599131452,
      "grad_norm": 3.9371821880340576,
      "learning_rate": 1.7193920160347423e-05,
      "loss": 1.3886,
      "step": 4200
    },
    {
      "epoch": 0.7182228161015534,
      "grad_norm": 5.721643447875977,
      "learning_rate": 1.712710873559379e-05,
      "loss": 1.3739,
      "step": 4300
    },
    {
      "epoch": 0.7349256722899615,
      "grad_norm": 3.645592451095581,
      "learning_rate": 1.7060297310840155e-05,
      "loss": 1.3691,
      "step": 4400
    },
    {
      "epoch": 0.7516285284783698,
      "grad_norm": 7.353744983673096,
      "learning_rate": 1.6993485886086524e-05,
      "loss": 1.3728,
      "step": 4500
    },
    {
      "epoch": 0.768331384666778,
      "grad_norm": 3.5684127807617188,
      "learning_rate": 1.692667446133289e-05,
      "loss": 1.3981,
      "step": 4600
    },
    {
      "epoch": 0.7850342408551862,
      "grad_norm": 3.5869362354278564,
      "learning_rate": 1.6859863036579256e-05,
      "loss": 1.3926,
      "step": 4700
    },
    {
      "epoch": 0.8017370970435944,
      "grad_norm": 5.691287994384766,
      "learning_rate": 1.6793051611825623e-05,
      "loss": 1.2854,
      "step": 4800
    },
    {
      "epoch": 0.8184399532320027,
      "grad_norm": 6.402614593505859,
      "learning_rate": 1.6726240187071992e-05,
      "loss": 1.4294,
      "step": 4900
    },
    {
      "epoch": 0.8351428094204109,
      "grad_norm": 5.367902755737305,
      "learning_rate": 1.6659428762318358e-05,
      "loss": 1.3952,
      "step": 5000
    },
    {
      "epoch": 0.8518456656088191,
      "grad_norm": 7.974998950958252,
      "learning_rate": 1.6592617337564724e-05,
      "loss": 1.4523,
      "step": 5100
    },
    {
      "epoch": 0.8685485217972273,
      "grad_norm": 4.5497565269470215,
      "learning_rate": 1.6525805912811094e-05,
      "loss": 1.385,
      "step": 5200
    },
    {
      "epoch": 0.8852513779856356,
      "grad_norm": 6.011530876159668,
      "learning_rate": 1.6458994488057457e-05,
      "loss": 1.4254,
      "step": 5300
    },
    {
      "epoch": 0.9019542341740437,
      "grad_norm": 5.519169330596924,
      "learning_rate": 1.6392183063303826e-05,
      "loss": 1.3982,
      "step": 5400
    },
    {
      "epoch": 0.918657090362452,
      "grad_norm": 6.942173480987549,
      "learning_rate": 1.6325371638550192e-05,
      "loss": 1.4059,
      "step": 5500
    },
    {
      "epoch": 0.9353599465508602,
      "grad_norm": 6.305017471313477,
      "learning_rate": 1.6258560213796562e-05,
      "loss": 1.3729,
      "step": 5600
    },
    {
      "epoch": 0.9520628027392685,
      "grad_norm": 4.437546253204346,
      "learning_rate": 1.6191748789042928e-05,
      "loss": 1.3981,
      "step": 5700
    },
    {
      "epoch": 0.9687656589276766,
      "grad_norm": 5.215808391571045,
      "learning_rate": 1.6124937364289294e-05,
      "loss": 1.3628,
      "step": 5800
    },
    {
      "epoch": 0.9854685151160848,
      "grad_norm": 5.135789394378662,
      "learning_rate": 1.6058125939535663e-05,
      "loss": 1.3975,
      "step": 5900
    },
    {
      "epoch": 1.002171371304493,
      "grad_norm": 6.109468936920166,
      "learning_rate": 1.599131451478203e-05,
      "loss": 1.4244,
      "step": 6000
    },
    {
      "epoch": 1.0188742274929012,
      "grad_norm": 6.076045989990234,
      "learning_rate": 1.5924503090028396e-05,
      "loss": 1.3536,
      "step": 6100
    },
    {
      "epoch": 1.0355770836813094,
      "grad_norm": 5.754726409912109,
      "learning_rate": 1.5857691665274765e-05,
      "loss": 1.4562,
      "step": 6200
    },
    {
      "epoch": 1.0522799398697178,
      "grad_norm": 7.474671840667725,
      "learning_rate": 1.579088024052113e-05,
      "loss": 1.4058,
      "step": 6300
    },
    {
      "epoch": 1.068982796058126,
      "grad_norm": 3.219611406326294,
      "learning_rate": 1.5724068815767497e-05,
      "loss": 1.4105,
      "step": 6400
    },
    {
      "epoch": 1.0856856522465341,
      "grad_norm": 5.344944000244141,
      "learning_rate": 1.5657257391013864e-05,
      "loss": 1.3953,
      "step": 6500
    },
    {
      "epoch": 1.1023885084349423,
      "grad_norm": 8.499120712280273,
      "learning_rate": 1.5590445966260233e-05,
      "loss": 1.397,
      "step": 6600
    },
    {
      "epoch": 1.1190913646233507,
      "grad_norm": 4.760041236877441,
      "learning_rate": 1.55236345415066e-05,
      "loss": 1.3668,
      "step": 6700
    },
    {
      "epoch": 1.1357942208117588,
      "grad_norm": 6.718495845794678,
      "learning_rate": 1.5456823116752965e-05,
      "loss": 1.4123,
      "step": 6800
    },
    {
      "epoch": 1.152497077000167,
      "grad_norm": 7.113610744476318,
      "learning_rate": 1.5390011691999335e-05,
      "loss": 1.4101,
      "step": 6900
    },
    {
      "epoch": 1.1691999331885752,
      "grad_norm": 3.4315152168273926,
      "learning_rate": 1.53232002672457e-05,
      "loss": 1.3381,
      "step": 7000
    },
    {
      "epoch": 1.1859027893769833,
      "grad_norm": 4.372231483459473,
      "learning_rate": 1.5256388842492067e-05,
      "loss": 1.3416,
      "step": 7100
    },
    {
      "epoch": 1.2026056455653917,
      "grad_norm": 5.004695892333984,
      "learning_rate": 1.5189577417738435e-05,
      "loss": 1.4339,
      "step": 7200
    },
    {
      "epoch": 1.2193085017538,
      "grad_norm": 5.981649398803711,
      "learning_rate": 1.5122765992984803e-05,
      "loss": 1.378,
      "step": 7300
    },
    {
      "epoch": 1.236011357942208,
      "grad_norm": 8.388871192932129,
      "learning_rate": 1.5055954568231169e-05,
      "loss": 1.3775,
      "step": 7400
    },
    {
      "epoch": 1.2527142141306165,
      "grad_norm": 3.6919519901275635,
      "learning_rate": 1.4989143143477537e-05,
      "loss": 1.4049,
      "step": 7500
    },
    {
      "epoch": 1.2694170703190246,
      "grad_norm": 4.482661247253418,
      "learning_rate": 1.4922331718723904e-05,
      "loss": 1.3217,
      "step": 7600
    },
    {
      "epoch": 1.2861199265074328,
      "grad_norm": 4.230072021484375,
      "learning_rate": 1.4855520293970269e-05,
      "loss": 1.3621,
      "step": 7700
    },
    {
      "epoch": 1.302822782695841,
      "grad_norm": 2.8322031497955322,
      "learning_rate": 1.4788708869216637e-05,
      "loss": 1.3343,
      "step": 7800
    },
    {
      "epoch": 1.3195256388842491,
      "grad_norm": 7.829510688781738,
      "learning_rate": 1.4721897444463004e-05,
      "loss": 1.3888,
      "step": 7900
    },
    {
      "epoch": 1.3362284950726575,
      "grad_norm": 4.757130146026611,
      "learning_rate": 1.4655086019709372e-05,
      "loss": 1.3974,
      "step": 8000
    },
    {
      "epoch": 1.3529313512610657,
      "grad_norm": 8.73408031463623,
      "learning_rate": 1.4588274594955738e-05,
      "loss": 1.3971,
      "step": 8100
    },
    {
      "epoch": 1.3696342074494738,
      "grad_norm": 4.053956985473633,
      "learning_rate": 1.4521463170202106e-05,
      "loss": 1.3933,
      "step": 8200
    },
    {
      "epoch": 1.386337063637882,
      "grad_norm": 5.764336109161377,
      "learning_rate": 1.4454651745448474e-05,
      "loss": 1.4382,
      "step": 8300
    },
    {
      "epoch": 1.4030399198262904,
      "grad_norm": 6.719440460205078,
      "learning_rate": 1.4387840320694838e-05,
      "loss": 1.3945,
      "step": 8400
    },
    {
      "epoch": 1.4197427760146986,
      "grad_norm": 7.841960430145264,
      "learning_rate": 1.4321028895941206e-05,
      "loss": 1.3528,
      "step": 8500
    },
    {
      "epoch": 1.4364456322031067,
      "grad_norm": 4.018834590911865,
      "learning_rate": 1.4254217471187574e-05,
      "loss": 1.3869,
      "step": 8600
    },
    {
      "epoch": 1.453148488391515,
      "grad_norm": 4.445382595062256,
      "learning_rate": 1.418740604643394e-05,
      "loss": 1.3698,
      "step": 8700
    },
    {
      "epoch": 1.469851344579923,
      "grad_norm": 6.909892559051514,
      "learning_rate": 1.4120594621680308e-05,
      "loss": 1.3954,
      "step": 8800
    },
    {
      "epoch": 1.4865542007683314,
      "grad_norm": 5.618646621704102,
      "learning_rate": 1.4053783196926676e-05,
      "loss": 1.3644,
      "step": 8900
    },
    {
      "epoch": 1.5032570569567396,
      "grad_norm": 5.198269367218018,
      "learning_rate": 1.3986971772173044e-05,
      "loss": 1.3479,
      "step": 9000
    },
    {
      "epoch": 1.5199599131451478,
      "grad_norm": 9.415607452392578,
      "learning_rate": 1.392016034741941e-05,
      "loss": 1.3872,
      "step": 9100
    },
    {
      "epoch": 1.5366627693335562,
      "grad_norm": 7.089500904083252,
      "learning_rate": 1.3853348922665777e-05,
      "loss": 1.3283,
      "step": 9200
    },
    {
      "epoch": 1.5533656255219643,
      "grad_norm": 5.676095962524414,
      "learning_rate": 1.3786537497912145e-05,
      "loss": 1.4087,
      "step": 9300
    },
    {
      "epoch": 1.5700684817103725,
      "grad_norm": 4.3542938232421875,
      "learning_rate": 1.371972607315851e-05,
      "loss": 1.3854,
      "step": 9400
    },
    {
      "epoch": 1.5867713378987807,
      "grad_norm": 3.2853591442108154,
      "learning_rate": 1.3652914648404877e-05,
      "loss": 1.4474,
      "step": 9500
    },
    {
      "epoch": 1.6034741940871888,
      "grad_norm": 7.084292888641357,
      "learning_rate": 1.3586103223651245e-05,
      "loss": 1.3818,
      "step": 9600
    },
    {
      "epoch": 1.620177050275597,
      "grad_norm": 5.368877410888672,
      "learning_rate": 1.3519291798897613e-05,
      "loss": 1.4349,
      "step": 9700
    },
    {
      "epoch": 1.6368799064640054,
      "grad_norm": 6.238716125488281,
      "learning_rate": 1.345248037414398e-05,
      "loss": 1.3763,
      "step": 9800
    },
    {
      "epoch": 1.6535827626524136,
      "grad_norm": 5.988050937652588,
      "learning_rate": 1.3385668949390347e-05,
      "loss": 1.3727,
      "step": 9900
    },
    {
      "epoch": 1.670285618840822,
      "grad_norm": 4.1837873458862305,
      "learning_rate": 1.3318857524636715e-05,
      "loss": 1.406,
      "step": 10000
    },
    {
      "epoch": 1.68698847502923,
      "grad_norm": 4.375498294830322,
      "learning_rate": 1.3252046099883081e-05,
      "loss": 1.428,
      "step": 10100
    },
    {
      "epoch": 1.7036913312176383,
      "grad_norm": 4.170604705810547,
      "learning_rate": 1.3185234675129449e-05,
      "loss": 1.3952,
      "step": 10200
    },
    {
      "epoch": 1.7203941874060464,
      "grad_norm": 6.903753757476807,
      "learning_rate": 1.3118423250375817e-05,
      "loss": 1.3642,
      "step": 10300
    },
    {
      "epoch": 1.7370970435944546,
      "grad_norm": 6.760815620422363,
      "learning_rate": 1.3051611825622184e-05,
      "loss": 1.3704,
      "step": 10400
    },
    {
      "epoch": 1.7537998997828628,
      "grad_norm": 4.300719738006592,
      "learning_rate": 1.2984800400868549e-05,
      "loss": 1.3971,
      "step": 10500
    },
    {
      "epoch": 1.770502755971271,
      "grad_norm": 8.978793144226074,
      "learning_rate": 1.2917988976114917e-05,
      "loss": 1.4292,
      "step": 10600
    },
    {
      "epoch": 1.7872056121596793,
      "grad_norm": 4.092706680297852,
      "learning_rate": 1.2851177551361284e-05,
      "loss": 1.4049,
      "step": 10700
    },
    {
      "epoch": 1.8039084683480875,
      "grad_norm": 4.981260299682617,
      "learning_rate": 1.278436612660765e-05,
      "loss": 1.429,
      "step": 10800
    },
    {
      "epoch": 1.8206113245364959,
      "grad_norm": 5.542999267578125,
      "learning_rate": 1.2717554701854018e-05,
      "loss": 1.3838,
      "step": 10900
    },
    {
      "epoch": 1.837314180724904,
      "grad_norm": 6.055586338043213,
      "learning_rate": 1.2650743277100386e-05,
      "loss": 1.3344,
      "step": 11000
    },
    {
      "epoch": 1.8540170369133122,
      "grad_norm": 3.6419246196746826,
      "learning_rate": 1.258393185234675e-05,
      "loss": 1.3801,
      "step": 11100
    },
    {
      "epoch": 1.8707198931017204,
      "grad_norm": 3.6451873779296875,
      "learning_rate": 1.2517120427593118e-05,
      "loss": 1.3548,
      "step": 11200
    },
    {
      "epoch": 1.8874227492901285,
      "grad_norm": 3.6407644748687744,
      "learning_rate": 1.2450309002839486e-05,
      "loss": 1.3692,
      "step": 11300
    },
    {
      "epoch": 1.9041256054785367,
      "grad_norm": 4.72261381149292,
      "learning_rate": 1.2383497578085854e-05,
      "loss": 1.3886,
      "step": 11400
    },
    {
      "epoch": 1.920828461666945,
      "grad_norm": 5.979846000671387,
      "learning_rate": 1.231668615333222e-05,
      "loss": 1.3888,
      "step": 11500
    },
    {
      "epoch": 1.9375313178553533,
      "grad_norm": 3.8365976810455322,
      "learning_rate": 1.2249874728578588e-05,
      "loss": 1.4087,
      "step": 11600
    },
    {
      "epoch": 1.9542341740437614,
      "grad_norm": 6.11507511138916,
      "learning_rate": 1.2183063303824956e-05,
      "loss": 1.3859,
      "step": 11700
    },
    {
      "epoch": 1.9709370302321698,
      "grad_norm": 6.381080627441406,
      "learning_rate": 1.2116251879071322e-05,
      "loss": 1.4484,
      "step": 11800
    },
    {
      "epoch": 1.987639886420578,
      "grad_norm": 4.300910472869873,
      "learning_rate": 1.204944045431769e-05,
      "loss": 1.3724,
      "step": 11900
    },
    {
      "epoch": 2.004342742608986,
      "grad_norm": 5.681706428527832,
      "learning_rate": 1.1982629029564057e-05,
      "loss": 1.3532,
      "step": 12000
    },
    {
      "epoch": 2.0210455987973943,
      "grad_norm": 6.402032852172852,
      "learning_rate": 1.1915817604810425e-05,
      "loss": 1.3912,
      "step": 12100
    },
    {
      "epoch": 2.0377484549858025,
      "grad_norm": 7.776414394378662,
      "learning_rate": 1.184900618005679e-05,
      "loss": 1.3402,
      "step": 12200
    },
    {
      "epoch": 2.0544513111742106,
      "grad_norm": 6.942835330963135,
      "learning_rate": 1.1782194755303157e-05,
      "loss": 1.3741,
      "step": 12300
    },
    {
      "epoch": 2.071154167362619,
      "grad_norm": 5.739211082458496,
      "learning_rate": 1.1715383330549525e-05,
      "loss": 1.3962,
      "step": 12400
    },
    {
      "epoch": 2.0878570235510274,
      "grad_norm": 3.578939914703369,
      "learning_rate": 1.1648571905795891e-05,
      "loss": 1.4037,
      "step": 12500
    },
    {
      "epoch": 2.1045598797394356,
      "grad_norm": 5.005422592163086,
      "learning_rate": 1.158176048104226e-05,
      "loss": 1.4592,
      "step": 12600
    },
    {
      "epoch": 2.1212627359278438,
      "grad_norm": 4.811509132385254,
      "learning_rate": 1.1514949056288627e-05,
      "loss": 1.3842,
      "step": 12700
    },
    {
      "epoch": 2.137965592116252,
      "grad_norm": 10.08215045928955,
      "learning_rate": 1.1448137631534993e-05,
      "loss": 1.3892,
      "step": 12800
    },
    {
      "epoch": 2.15466844830466,
      "grad_norm": 7.535407066345215,
      "learning_rate": 1.1381326206781361e-05,
      "loss": 1.3476,
      "step": 12900
    },
    {
      "epoch": 2.1713713044930683,
      "grad_norm": 8.672653198242188,
      "learning_rate": 1.1314514782027727e-05,
      "loss": 1.3823,
      "step": 13000
    },
    {
      "epoch": 2.1880741606814764,
      "grad_norm": 4.666319847106934,
      "learning_rate": 1.1247703357274095e-05,
      "loss": 1.3801,
      "step": 13100
    },
    {
      "epoch": 2.2047770168698846,
      "grad_norm": 6.741872787475586,
      "learning_rate": 1.1180891932520461e-05,
      "loss": 1.3795,
      "step": 13200
    },
    {
      "epoch": 2.2214798730582928,
      "grad_norm": 5.15491247177124,
      "learning_rate": 1.1114080507766829e-05,
      "loss": 1.4249,
      "step": 13300
    },
    {
      "epoch": 2.2381827292467014,
      "grad_norm": 9.824100494384766,
      "learning_rate": 1.1047269083013197e-05,
      "loss": 1.3739,
      "step": 13400
    },
    {
      "epoch": 2.2548855854351095,
      "grad_norm": 10.482579231262207,
      "learning_rate": 1.0980457658259563e-05,
      "loss": 1.3886,
      "step": 13500
    },
    {
      "epoch": 2.2715884416235177,
      "grad_norm": 5.0249433517456055,
      "learning_rate": 1.091364623350593e-05,
      "loss": 1.373,
      "step": 13600
    },
    {
      "epoch": 2.288291297811926,
      "grad_norm": 3.5472309589385986,
      "learning_rate": 1.0846834808752298e-05,
      "loss": 1.4139,
      "step": 13700
    },
    {
      "epoch": 2.304994154000334,
      "grad_norm": 6.234273910522461,
      "learning_rate": 1.0780023383998666e-05,
      "loss": 1.4075,
      "step": 13800
    },
    {
      "epoch": 2.321697010188742,
      "grad_norm": 5.176645755767822,
      "learning_rate": 1.071321195924503e-05,
      "loss": 1.3945,
      "step": 13900
    },
    {
      "epoch": 2.3383998663771504,
      "grad_norm": 4.6981282234191895,
      "learning_rate": 1.0646400534491398e-05,
      "loss": 1.3904,
      "step": 14000
    },
    {
      "epoch": 2.3551027225655585,
      "grad_norm": 5.258945465087891,
      "learning_rate": 1.0579589109737766e-05,
      "loss": 1.3919,
      "step": 14100
    },
    {
      "epoch": 2.3718055787539667,
      "grad_norm": 4.275466442108154,
      "learning_rate": 1.0512777684984132e-05,
      "loss": 1.367,
      "step": 14200
    },
    {
      "epoch": 2.3885084349423753,
      "grad_norm": 5.318302631378174,
      "learning_rate": 1.04459662602305e-05,
      "loss": 1.351,
      "step": 14300
    },
    {
      "epoch": 2.4052112911307835,
      "grad_norm": 7.0374755859375,
      "learning_rate": 1.0379154835476868e-05,
      "loss": 1.4114,
      "step": 14400
    },
    {
      "epoch": 2.4219141473191916,
      "grad_norm": 8.08237075805664,
      "learning_rate": 1.0312343410723234e-05,
      "loss": 1.4027,
      "step": 14500
    },
    {
      "epoch": 2.4386170035076,
      "grad_norm": 6.5107011795043945,
      "learning_rate": 1.0245531985969602e-05,
      "loss": 1.4177,
      "step": 14600
    },
    {
      "epoch": 2.455319859696008,
      "grad_norm": 4.2943806648254395,
      "learning_rate": 1.017872056121597e-05,
      "loss": 1.4239,
      "step": 14700
    },
    {
      "epoch": 2.472022715884416,
      "grad_norm": 2.9367001056671143,
      "learning_rate": 1.0111909136462337e-05,
      "loss": 1.4143,
      "step": 14800
    },
    {
      "epoch": 2.4887255720728243,
      "grad_norm": 5.687348365783691,
      "learning_rate": 1.0045097711708702e-05,
      "loss": 1.3756,
      "step": 14900
    },
    {
      "epoch": 2.505428428261233,
      "grad_norm": 6.471633434295654,
      "learning_rate": 9.97828628695507e-06,
      "loss": 1.3486,
      "step": 15000
    },
    {
      "epoch": 2.5221312844496406,
      "grad_norm": 3.6519837379455566,
      "learning_rate": 9.911474862201438e-06,
      "loss": 1.3347,
      "step": 15100
    },
    {
      "epoch": 2.5388341406380492,
      "grad_norm": 3.703078508377075,
      "learning_rate": 9.844663437447805e-06,
      "loss": 1.3883,
      "step": 15200
    },
    {
      "epoch": 2.5555369968264574,
      "grad_norm": 6.963004112243652,
      "learning_rate": 9.777852012694171e-06,
      "loss": 1.3794,
      "step": 15300
    },
    {
      "epoch": 2.5722398530148656,
      "grad_norm": 3.8110191822052,
      "learning_rate": 9.71104058794054e-06,
      "loss": 1.3387,
      "step": 15400
    },
    {
      "epoch": 2.5889427092032737,
      "grad_norm": 4.75086784362793,
      "learning_rate": 9.644229163186905e-06,
      "loss": 1.377,
      "step": 15500
    },
    {
      "epoch": 2.605645565391682,
      "grad_norm": 5.0202484130859375,
      "learning_rate": 9.577417738433273e-06,
      "loss": 1.4211,
      "step": 15600
    },
    {
      "epoch": 2.62234842158009,
      "grad_norm": 5.283592224121094,
      "learning_rate": 9.51060631367964e-06,
      "loss": 1.3959,
      "step": 15700
    },
    {
      "epoch": 2.6390512777684982,
      "grad_norm": 4.396175384521484,
      "learning_rate": 9.443794888926007e-06,
      "loss": 1.3816,
      "step": 15800
    },
    {
      "epoch": 2.655754133956907,
      "grad_norm": 9.436023712158203,
      "learning_rate": 9.376983464172375e-06,
      "loss": 1.3575,
      "step": 15900
    },
    {
      "epoch": 2.672456990145315,
      "grad_norm": 2.954601526260376,
      "learning_rate": 9.310172039418741e-06,
      "loss": 1.3174,
      "step": 16000
    },
    {
      "epoch": 2.689159846333723,
      "grad_norm": 4.678534507751465,
      "learning_rate": 9.243360614665109e-06,
      "loss": 1.4107,
      "step": 16100
    },
    {
      "epoch": 2.7058627025221313,
      "grad_norm": 4.778240203857422,
      "learning_rate": 9.176549189911475e-06,
      "loss": 1.3943,
      "step": 16200
    },
    {
      "epoch": 2.7225655587105395,
      "grad_norm": 9.23222541809082,
      "learning_rate": 9.109737765157843e-06,
      "loss": 1.3894,
      "step": 16300
    },
    {
      "epoch": 2.7392684148989477,
      "grad_norm": 5.982388973236084,
      "learning_rate": 9.04292634040421e-06,
      "loss": 1.3724,
      "step": 16400
    },
    {
      "epoch": 2.755971271087356,
      "grad_norm": 2.375950813293457,
      "learning_rate": 8.976114915650577e-06,
      "loss": 1.4603,
      "step": 16500
    },
    {
      "epoch": 2.772674127275764,
      "grad_norm": 4.705142974853516,
      "learning_rate": 8.909303490896945e-06,
      "loss": 1.4274,
      "step": 16600
    },
    {
      "epoch": 2.789376983464172,
      "grad_norm": 4.591125011444092,
      "learning_rate": 8.84249206614331e-06,
      "loss": 1.3436,
      "step": 16700
    },
    {
      "epoch": 2.806079839652581,
      "grad_norm": 6.612968921661377,
      "learning_rate": 8.775680641389678e-06,
      "loss": 1.3716,
      "step": 16800
    },
    {
      "epoch": 2.822782695840989,
      "grad_norm": 4.020537376403809,
      "learning_rate": 8.708869216636046e-06,
      "loss": 1.39,
      "step": 16900
    },
    {
      "epoch": 2.839485552029397,
      "grad_norm": 5.737396240234375,
      "learning_rate": 8.642057791882412e-06,
      "loss": 1.3251,
      "step": 17000
    },
    {
      "epoch": 2.8561884082178053,
      "grad_norm": 7.240434169769287,
      "learning_rate": 8.57524636712878e-06,
      "loss": 1.3888,
      "step": 17100
    },
    {
      "epoch": 2.8728912644062135,
      "grad_norm": 3.6467130184173584,
      "learning_rate": 8.508434942375146e-06,
      "loss": 1.382,
      "step": 17200
    },
    {
      "epoch": 2.8895941205946216,
      "grad_norm": 6.334061622619629,
      "learning_rate": 8.441623517621514e-06,
      "loss": 1.3705,
      "step": 17300
    },
    {
      "epoch": 2.90629697678303,
      "grad_norm": 3.904435634613037,
      "learning_rate": 8.374812092867882e-06,
      "loss": 1.3835,
      "step": 17400
    },
    {
      "epoch": 2.922999832971438,
      "grad_norm": 6.697598457336426,
      "learning_rate": 8.308000668114248e-06,
      "loss": 1.4099,
      "step": 17500
    },
    {
      "epoch": 2.939702689159846,
      "grad_norm": 5.556886672973633,
      "learning_rate": 8.241189243360616e-06,
      "loss": 1.3846,
      "step": 17600
    },
    {
      "epoch": 2.9564055453482547,
      "grad_norm": 4.72573709487915,
      "learning_rate": 8.174377818606982e-06,
      "loss": 1.4205,
      "step": 17700
    },
    {
      "epoch": 2.973108401536663,
      "grad_norm": 4.660336971282959,
      "learning_rate": 8.10756639385335e-06,
      "loss": 1.4066,
      "step": 17800
    },
    {
      "epoch": 2.989811257725071,
      "grad_norm": 4.763261318206787,
      "learning_rate": 8.040754969099716e-06,
      "loss": 1.3713,
      "step": 17900
    },
    {
      "epoch": 3.0065141139134792,
      "grad_norm": 7.221831321716309,
      "learning_rate": 7.973943544346084e-06,
      "loss": 1.3732,
      "step": 18000
    },
    {
      "epoch": 3.0232169701018874,
      "grad_norm": 6.099342346191406,
      "learning_rate": 7.907132119592451e-06,
      "loss": 1.3413,
      "step": 18100
    },
    {
      "epoch": 3.0399198262902956,
      "grad_norm": 6.046839237213135,
      "learning_rate": 7.840320694838818e-06,
      "loss": 1.3762,
      "step": 18200
    },
    {
      "epoch": 3.0566226824787037,
      "grad_norm": 5.431028842926025,
      "learning_rate": 7.773509270085185e-06,
      "loss": 1.3929,
      "step": 18300
    },
    {
      "epoch": 3.073325538667112,
      "grad_norm": 4.755274295806885,
      "learning_rate": 7.706697845331552e-06,
      "loss": 1.3423,
      "step": 18400
    },
    {
      "epoch": 3.0900283948555205,
      "grad_norm": 5.130277633666992,
      "learning_rate": 7.63988642057792e-06,
      "loss": 1.3291,
      "step": 18500
    },
    {
      "epoch": 3.1067312510439287,
      "grad_norm": 4.90659236907959,
      "learning_rate": 7.573074995824286e-06,
      "loss": 1.3764,
      "step": 18600
    },
    {
      "epoch": 3.123434107232337,
      "grad_norm": 7.948573112487793,
      "learning_rate": 7.506263571070653e-06,
      "loss": 1.4181,
      "step": 18700
    },
    {
      "epoch": 3.140136963420745,
      "grad_norm": 5.151589870452881,
      "learning_rate": 7.439452146317021e-06,
      "loss": 1.4579,
      "step": 18800
    },
    {
      "epoch": 3.156839819609153,
      "grad_norm": 4.570492267608643,
      "learning_rate": 7.372640721563388e-06,
      "loss": 1.4284,
      "step": 18900
    },
    {
      "epoch": 3.1735426757975613,
      "grad_norm": 6.501192569732666,
      "learning_rate": 7.305829296809756e-06,
      "loss": 1.3614,
      "step": 19000
    },
    {
      "epoch": 3.1902455319859695,
      "grad_norm": 4.726883888244629,
      "learning_rate": 7.239017872056122e-06,
      "loss": 1.3964,
      "step": 19100
    },
    {
      "epoch": 3.2069483881743777,
      "grad_norm": 4.606996059417725,
      "learning_rate": 7.172206447302489e-06,
      "loss": 1.3882,
      "step": 19200
    },
    {
      "epoch": 3.2236512443627863,
      "grad_norm": 4.400814533233643,
      "learning_rate": 7.105395022548857e-06,
      "loss": 1.3693,
      "step": 19300
    },
    {
      "epoch": 3.2403541005511944,
      "grad_norm": 6.5517730712890625,
      "learning_rate": 7.038583597795223e-06,
      "loss": 1.4085,
      "step": 19400
    },
    {
      "epoch": 3.2570569567396026,
      "grad_norm": 12.26562786102295,
      "learning_rate": 6.971772173041591e-06,
      "loss": 1.3369,
      "step": 19500
    },
    {
      "epoch": 3.2737598129280108,
      "grad_norm": 2.823810577392578,
      "learning_rate": 6.904960748287958e-06,
      "loss": 1.3486,
      "step": 19600
    },
    {
      "epoch": 3.290462669116419,
      "grad_norm": 4.20294713973999,
      "learning_rate": 6.8381493235343254e-06,
      "loss": 1.3796,
      "step": 19700
    },
    {
      "epoch": 3.307165525304827,
      "grad_norm": 5.7986602783203125,
      "learning_rate": 6.771337898780692e-06,
      "loss": 1.4398,
      "step": 19800
    },
    {
      "epoch": 3.3238683814932353,
      "grad_norm": 2.826127767562866,
      "learning_rate": 6.7045264740270585e-06,
      "loss": 1.38,
      "step": 19900
    },
    {
      "epoch": 3.3405712376816434,
      "grad_norm": 6.859213829040527,
      "learning_rate": 6.637715049273426e-06,
      "loss": 1.3963,
      "step": 20000
    },
    {
      "epoch": 3.3572740938700516,
      "grad_norm": 8.322518348693848,
      "learning_rate": 6.570903624519793e-06,
      "loss": 1.3445,
      "step": 20100
    },
    {
      "epoch": 3.37397695005846,
      "grad_norm": 3.8713326454162598,
      "learning_rate": 6.504092199766161e-06,
      "loss": 1.4303,
      "step": 20200
    },
    {
      "epoch": 3.3906798062468684,
      "grad_norm": 5.350264072418213,
      "learning_rate": 6.437280775012528e-06,
      "loss": 1.3621,
      "step": 20300
    },
    {
      "epoch": 3.4073826624352765,
      "grad_norm": 3.5799458026885986,
      "learning_rate": 6.370469350258894e-06,
      "loss": 1.3806,
      "step": 20400
    },
    {
      "epoch": 3.4240855186236847,
      "grad_norm": 6.730466365814209,
      "learning_rate": 6.303657925505262e-06,
      "loss": 1.3549,
      "step": 20500
    },
    {
      "epoch": 3.440788374812093,
      "grad_norm": 5.732527732849121,
      "learning_rate": 6.236846500751629e-06,
      "loss": 1.3876,
      "step": 20600
    },
    {
      "epoch": 3.457491231000501,
      "grad_norm": 4.746913909912109,
      "learning_rate": 6.170035075997997e-06,
      "loss": 1.4186,
      "step": 20700
    },
    {
      "epoch": 3.474194087188909,
      "grad_norm": 5.223557949066162,
      "learning_rate": 6.103223651244363e-06,
      "loss": 1.4055,
      "step": 20800
    },
    {
      "epoch": 3.4908969433773174,
      "grad_norm": 5.370590686798096,
      "learning_rate": 6.03641222649073e-06,
      "loss": 1.3596,
      "step": 20900
    },
    {
      "epoch": 3.5075997995657255,
      "grad_norm": 4.778830528259277,
      "learning_rate": 5.969600801737098e-06,
      "loss": 1.3789,
      "step": 21000
    },
    {
      "epoch": 3.524302655754134,
      "grad_norm": 3.5083260536193848,
      "learning_rate": 5.902789376983465e-06,
      "loss": 1.3988,
      "step": 21100
    },
    {
      "epoch": 3.5410055119425423,
      "grad_norm": 4.260839462280273,
      "learning_rate": 5.835977952229832e-06,
      "loss": 1.3886,
      "step": 21200
    },
    {
      "epoch": 3.5577083681309505,
      "grad_norm": 4.629791736602783,
      "learning_rate": 5.7691665274761985e-06,
      "loss": 1.3692,
      "step": 21300
    },
    {
      "epoch": 3.5744112243193586,
      "grad_norm": 5.545022964477539,
      "learning_rate": 5.702355102722566e-06,
      "loss": 1.4222,
      "step": 21400
    },
    {
      "epoch": 3.591114080507767,
      "grad_norm": 3.244682550430298,
      "learning_rate": 5.635543677968933e-06,
      "loss": 1.3672,
      "step": 21500
    },
    {
      "epoch": 3.607816936696175,
      "grad_norm": 5.301467418670654,
      "learning_rate": 5.5687322532153e-06,
      "loss": 1.4387,
      "step": 21600
    },
    {
      "epoch": 3.624519792884583,
      "grad_norm": 7.078131198883057,
      "learning_rate": 5.501920828461667e-06,
      "loss": 1.3506,
      "step": 21700
    },
    {
      "epoch": 3.6412226490729918,
      "grad_norm": 4.652040004730225,
      "learning_rate": 5.435109403708034e-06,
      "loss": 1.3998,
      "step": 21800
    },
    {
      "epoch": 3.6579255052613995,
      "grad_norm": 5.1828742027282715,
      "learning_rate": 5.368297978954402e-06,
      "loss": 1.43,
      "step": 21900
    },
    {
      "epoch": 3.674628361449808,
      "grad_norm": 4.483330249786377,
      "learning_rate": 5.301486554200769e-06,
      "loss": 1.4149,
      "step": 22000
    },
    {
      "epoch": 3.6913312176382163,
      "grad_norm": 5.1892170906066895,
      "learning_rate": 5.234675129447135e-06,
      "loss": 1.4067,
      "step": 22100
    },
    {
      "epoch": 3.7080340738266244,
      "grad_norm": 4.696274757385254,
      "learning_rate": 5.167863704693503e-06,
      "loss": 1.4208,
      "step": 22200
    },
    {
      "epoch": 3.7247369300150326,
      "grad_norm": 4.500481128692627,
      "learning_rate": 5.10105227993987e-06,
      "loss": 1.3606,
      "step": 22300
    },
    {
      "epoch": 3.7414397862034408,
      "grad_norm": 7.619372844696045,
      "learning_rate": 5.034240855186238e-06,
      "loss": 1.3425,
      "step": 22400
    },
    {
      "epoch": 3.758142642391849,
      "grad_norm": 5.314615726470947,
      "learning_rate": 4.967429430432605e-06,
      "loss": 1.369,
      "step": 22500
    },
    {
      "epoch": 3.774845498580257,
      "grad_norm": 6.25777530670166,
      "learning_rate": 4.9006180056789716e-06,
      "loss": 1.3752,
      "step": 22600
    },
    {
      "epoch": 3.7915483547686657,
      "grad_norm": 6.737982273101807,
      "learning_rate": 4.8338065809253385e-06,
      "loss": 1.3846,
      "step": 22700
    },
    {
      "epoch": 3.8082512109570734,
      "grad_norm": 4.049083709716797,
      "learning_rate": 4.766995156171706e-06,
      "loss": 1.4134,
      "step": 22800
    },
    {
      "epoch": 3.824954067145482,
      "grad_norm": 7.036305904388428,
      "learning_rate": 4.7001837314180725e-06,
      "loss": 1.3705,
      "step": 22900
    },
    {
      "epoch": 3.84165692333389,
      "grad_norm": 2.4330155849456787,
      "learning_rate": 4.633372306664439e-06,
      "loss": 1.3199,
      "step": 23000
    },
    {
      "epoch": 3.8583597795222984,
      "grad_norm": 2.8162782192230225,
      "learning_rate": 4.566560881910807e-06,
      "loss": 1.3887,
      "step": 23100
    },
    {
      "epoch": 3.8750626357107065,
      "grad_norm": 8.596085548400879,
      "learning_rate": 4.499749457157174e-06,
      "loss": 1.4086,
      "step": 23200
    },
    {
      "epoch": 3.8917654918991147,
      "grad_norm": 7.815463066101074,
      "learning_rate": 4.432938032403541e-06,
      "loss": 1.3644,
      "step": 23300
    },
    {
      "epoch": 3.908468348087523,
      "grad_norm": 7.05710506439209,
      "learning_rate": 4.366126607649909e-06,
      "loss": 1.3785,
      "step": 23400
    },
    {
      "epoch": 3.925171204275931,
      "grad_norm": 8.66176986694336,
      "learning_rate": 4.299315182896275e-06,
      "loss": 1.4025,
      "step": 23500
    },
    {
      "epoch": 3.9418740604643396,
      "grad_norm": 3.754633665084839,
      "learning_rate": 4.232503758142643e-06,
      "loss": 1.3689,
      "step": 23600
    },
    {
      "epoch": 3.9585769166527474,
      "grad_norm": 4.5104804039001465,
      "learning_rate": 4.16569233338901e-06,
      "loss": 1.4299,
      "step": 23700
    },
    {
      "epoch": 3.975279772841156,
      "grad_norm": 4.93242073059082,
      "learning_rate": 4.098880908635377e-06,
      "loss": 1.3918,
      "step": 23800
    },
    {
      "epoch": 3.991982629029564,
      "grad_norm": 4.916132926940918,
      "learning_rate": 4.032069483881745e-06,
      "loss": 1.3764,
      "step": 23900
    },
    {
      "epoch": 4.008685485217972,
      "grad_norm": 3.900925636291504,
      "learning_rate": 3.965258059128112e-06,
      "loss": 1.3163,
      "step": 24000
    },
    {
      "epoch": 4.025388341406381,
      "grad_norm": 4.684047698974609,
      "learning_rate": 3.898446634374478e-06,
      "loss": 1.3637,
      "step": 24100
    },
    {
      "epoch": 4.042091197594789,
      "grad_norm": 5.593021869659424,
      "learning_rate": 3.8316352096208455e-06,
      "loss": 1.4018,
      "step": 24200
    },
    {
      "epoch": 4.058794053783197,
      "grad_norm": 10.80580997467041,
      "learning_rate": 3.7648237848672125e-06,
      "loss": 1.3434,
      "step": 24300
    },
    {
      "epoch": 4.075496909971605,
      "grad_norm": 4.775548934936523,
      "learning_rate": 3.69801236011358e-06,
      "loss": 1.324,
      "step": 24400
    },
    {
      "epoch": 4.092199766160014,
      "grad_norm": 2.591341495513916,
      "learning_rate": 3.631200935359947e-06,
      "loss": 1.352,
      "step": 24500
    },
    {
      "epoch": 4.108902622348421,
      "grad_norm": 3.3586673736572266,
      "learning_rate": 3.564389510606314e-06,
      "loss": 1.3539,
      "step": 24600
    },
    {
      "epoch": 4.12560547853683,
      "grad_norm": 4.410589694976807,
      "learning_rate": 3.4975780858526807e-06,
      "loss": 1.4272,
      "step": 24700
    },
    {
      "epoch": 4.142308334725238,
      "grad_norm": 5.442556381225586,
      "learning_rate": 3.430766661099048e-06,
      "loss": 1.395,
      "step": 24800
    },
    {
      "epoch": 4.159011190913646,
      "grad_norm": 4.304095268249512,
      "learning_rate": 3.3639552363454155e-06,
      "loss": 1.3385,
      "step": 24900
    },
    {
      "epoch": 4.175714047102055,
      "grad_norm": 8.205086708068848,
      "learning_rate": 3.2971438115917825e-06,
      "loss": 1.3975,
      "step": 25000
    },
    {
      "epoch": 4.192416903290463,
      "grad_norm": 5.639706611633301,
      "learning_rate": 3.23033238683815e-06,
      "loss": 1.3594,
      "step": 25100
    },
    {
      "epoch": 4.209119759478871,
      "grad_norm": 3.6864984035491943,
      "learning_rate": 3.163520962084517e-06,
      "loss": 1.3991,
      "step": 25200
    },
    {
      "epoch": 4.225822615667279,
      "grad_norm": 5.415999889373779,
      "learning_rate": 3.096709537330884e-06,
      "loss": 1.378,
      "step": 25300
    },
    {
      "epoch": 4.2425254718556875,
      "grad_norm": 4.243735313415527,
      "learning_rate": 3.0298981125772508e-06,
      "loss": 1.3817,
      "step": 25400
    },
    {
      "epoch": 4.259228328044095,
      "grad_norm": 4.558913230895996,
      "learning_rate": 2.963086687823618e-06,
      "loss": 1.3819,
      "step": 25500
    },
    {
      "epoch": 4.275931184232504,
      "grad_norm": 5.052678108215332,
      "learning_rate": 2.896275263069985e-06,
      "loss": 1.3504,
      "step": 25600
    },
    {
      "epoch": 4.292634040420912,
      "grad_norm": 5.279711723327637,
      "learning_rate": 2.8294638383163525e-06,
      "loss": 1.3812,
      "step": 25700
    },
    {
      "epoch": 4.30933689660932,
      "grad_norm": 5.793715953826904,
      "learning_rate": 2.762652413562719e-06,
      "loss": 1.4241,
      "step": 25800
    },
    {
      "epoch": 4.326039752797729,
      "grad_norm": 6.44719123840332,
      "learning_rate": 2.6958409888090864e-06,
      "loss": 1.3842,
      "step": 25900
    },
    {
      "epoch": 4.3427426089861365,
      "grad_norm": 4.65890645980835,
      "learning_rate": 2.629029564055454e-06,
      "loss": 1.4213,
      "step": 26000
    },
    {
      "epoch": 4.359445465174545,
      "grad_norm": 6.758935928344727,
      "learning_rate": 2.5622181393018208e-06,
      "loss": 1.3704,
      "step": 26100
    },
    {
      "epoch": 4.376148321362953,
      "grad_norm": 5.2372894287109375,
      "learning_rate": 2.495406714548188e-06,
      "loss": 1.3991,
      "step": 26200
    },
    {
      "epoch": 4.3928511775513615,
      "grad_norm": 6.292636394500732,
      "learning_rate": 2.428595289794555e-06,
      "loss": 1.3741,
      "step": 26300
    },
    {
      "epoch": 4.409554033739769,
      "grad_norm": 7.498908996582031,
      "learning_rate": 2.361783865040922e-06,
      "loss": 1.3849,
      "step": 26400
    },
    {
      "epoch": 4.426256889928178,
      "grad_norm": 4.812742710113525,
      "learning_rate": 2.2949724402872895e-06,
      "loss": 1.3653,
      "step": 26500
    },
    {
      "epoch": 4.4429597461165855,
      "grad_norm": 4.382757186889648,
      "learning_rate": 2.2281610155336564e-06,
      "loss": 1.3611,
      "step": 26600
    },
    {
      "epoch": 4.459662602304994,
      "grad_norm": 8.349162101745605,
      "learning_rate": 2.161349590780024e-06,
      "loss": 1.405,
      "step": 26700
    },
    {
      "epoch": 4.476365458493403,
      "grad_norm": 4.220862865447998,
      "learning_rate": 2.0945381660263908e-06,
      "loss": 1.4029,
      "step": 26800
    },
    {
      "epoch": 4.4930683146818104,
      "grad_norm": 5.7247538566589355,
      "learning_rate": 2.0277267412727577e-06,
      "loss": 1.443,
      "step": 26900
    },
    {
      "epoch": 4.509771170870219,
      "grad_norm": 4.721450328826904,
      "learning_rate": 1.960915316519125e-06,
      "loss": 1.3147,
      "step": 27000
    },
    {
      "epoch": 4.526474027058627,
      "grad_norm": 6.550407409667969,
      "learning_rate": 1.8941038917654919e-06,
      "loss": 1.3528,
      "step": 27100
    },
    {
      "epoch": 4.543176883247035,
      "grad_norm": 4.822470188140869,
      "learning_rate": 1.827292467011859e-06,
      "loss": 1.3876,
      "step": 27200
    },
    {
      "epoch": 4.559879739435443,
      "grad_norm": 5.338608741760254,
      "learning_rate": 1.7604810422582264e-06,
      "loss": 1.4222,
      "step": 27300
    },
    {
      "epoch": 4.576582595623852,
      "grad_norm": 3.723984718322754,
      "learning_rate": 1.6936696175045934e-06,
      "loss": 1.3577,
      "step": 27400
    },
    {
      "epoch": 4.59328545181226,
      "grad_norm": 4.3714280128479,
      "learning_rate": 1.6268581927509606e-06,
      "loss": 1.4187,
      "step": 27500
    },
    {
      "epoch": 4.609988308000668,
      "grad_norm": 4.609213352203369,
      "learning_rate": 1.5600467679973277e-06,
      "loss": 1.3738,
      "step": 27600
    },
    {
      "epoch": 4.626691164189077,
      "grad_norm": 4.230288982391357,
      "learning_rate": 1.4932353432436947e-06,
      "loss": 1.3807,
      "step": 27700
    },
    {
      "epoch": 4.643394020377484,
      "grad_norm": 4.880049705505371,
      "learning_rate": 1.4264239184900619e-06,
      "loss": 1.4097,
      "step": 27800
    },
    {
      "epoch": 4.660096876565893,
      "grad_norm": 7.342586517333984,
      "learning_rate": 1.359612493736429e-06,
      "loss": 1.3465,
      "step": 27900
    },
    {
      "epoch": 4.676799732754301,
      "grad_norm": 4.555983066558838,
      "learning_rate": 1.292801068982796e-06,
      "loss": 1.3739,
      "step": 28000
    },
    {
      "epoch": 4.693502588942709,
      "grad_norm": 5.100472450256348,
      "learning_rate": 1.2259896442291632e-06,
      "loss": 1.3468,
      "step": 28100
    },
    {
      "epoch": 4.710205445131117,
      "grad_norm": 4.682727813720703,
      "learning_rate": 1.1591782194755304e-06,
      "loss": 1.3637,
      "step": 28200
    },
    {
      "epoch": 4.726908301319526,
      "grad_norm": 5.836568355560303,
      "learning_rate": 1.0923667947218975e-06,
      "loss": 1.4075,
      "step": 28300
    },
    {
      "epoch": 4.743611157507933,
      "grad_norm": 5.873263835906982,
      "learning_rate": 1.0255553699682647e-06,
      "loss": 1.3739,
      "step": 28400
    },
    {
      "epoch": 4.760314013696342,
      "grad_norm": 3.8792474269866943,
      "learning_rate": 9.587439452146317e-07,
      "loss": 1.3934,
      "step": 28500
    },
    {
      "epoch": 4.777016869884751,
      "grad_norm": 4.1465301513671875,
      "learning_rate": 8.91932520460999e-07,
      "loss": 1.4125,
      "step": 28600
    },
    {
      "epoch": 4.793719726073158,
      "grad_norm": 5.826981067657471,
      "learning_rate": 8.25121095707366e-07,
      "loss": 1.436,
      "step": 28700
    },
    {
      "epoch": 4.810422582261567,
      "grad_norm": 6.22089958190918,
      "learning_rate": 7.583096709537331e-07,
      "loss": 1.4026,
      "step": 28800
    },
    {
      "epoch": 4.827125438449975,
      "grad_norm": 3.525286912918091,
      "learning_rate": 6.914982462001004e-07,
      "loss": 1.4114,
      "step": 28900
    },
    {
      "epoch": 4.843828294638383,
      "grad_norm": 5.588958740234375,
      "learning_rate": 6.246868214464674e-07,
      "loss": 1.3407,
      "step": 29000
    },
    {
      "epoch": 4.860531150826791,
      "grad_norm": 3.968794345855713,
      "learning_rate": 5.578753966928345e-07,
      "loss": 1.4245,
      "step": 29100
    },
    {
      "epoch": 4.8772340070152,
      "grad_norm": 5.270242214202881,
      "learning_rate": 4.910639719392017e-07,
      "loss": 1.4216,
      "step": 29200
    },
    {
      "epoch": 4.893936863203608,
      "grad_norm": 4.484533786773682,
      "learning_rate": 4.242525471855688e-07,
      "loss": 1.3743,
      "step": 29300
    },
    {
      "epoch": 4.910639719392016,
      "grad_norm": 4.8584113121032715,
      "learning_rate": 3.5744112243193587e-07,
      "loss": 1.3949,
      "step": 29400
    },
    {
      "epoch": 4.9273425755804245,
      "grad_norm": 4.80565881729126,
      "learning_rate": 2.9062969767830305e-07,
      "loss": 1.4309,
      "step": 29500
    }
  ],
  "logging_steps": 100,
  "max_steps": 29935,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 4.123432641705062e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
