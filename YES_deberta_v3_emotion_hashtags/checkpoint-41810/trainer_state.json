{
  "best_metric": 0.812039124462268,
  "best_model_checkpoint": "./deberta_v3_emotion_hashtags\\checkpoint-8362",
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 41810,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01195886151638364,
      "grad_norm": 10.08096981048584,
      "learning_rate": 1.995216455393447e-05,
      "loss": 1.5546,
      "step": 100
    },
    {
      "epoch": 0.02391772303276728,
      "grad_norm": 14.042702674865723,
      "learning_rate": 1.9904329107868932e-05,
      "loss": 1.2617,
      "step": 200
    },
    {
      "epoch": 0.03587658454915092,
      "grad_norm": 9.697357177734375,
      "learning_rate": 1.98564936618034e-05,
      "loss": 1.0324,
      "step": 300
    },
    {
      "epoch": 0.04783544606553456,
      "grad_norm": 10.099104881286621,
      "learning_rate": 1.9808658215737863e-05,
      "loss": 0.8648,
      "step": 400
    },
    {
      "epoch": 0.0597943075819182,
      "grad_norm": 15.621537208557129,
      "learning_rate": 1.976082276967233e-05,
      "loss": 0.8629,
      "step": 500
    },
    {
      "epoch": 0.07175316909830184,
      "grad_norm": 7.644753932952881,
      "learning_rate": 1.9712987323606793e-05,
      "loss": 0.8331,
      "step": 600
    },
    {
      "epoch": 0.08371203061468548,
      "grad_norm": 13.018353462219238,
      "learning_rate": 1.966515187754126e-05,
      "loss": 0.8118,
      "step": 700
    },
    {
      "epoch": 0.09567089213106912,
      "grad_norm": 34.868568420410156,
      "learning_rate": 1.9617316431475727e-05,
      "loss": 0.7464,
      "step": 800
    },
    {
      "epoch": 0.10762975364745277,
      "grad_norm": 19.370105743408203,
      "learning_rate": 1.956948098541019e-05,
      "loss": 0.7588,
      "step": 900
    },
    {
      "epoch": 0.1195886151638364,
      "grad_norm": 20.716096878051758,
      "learning_rate": 1.9521645539344657e-05,
      "loss": 0.708,
      "step": 1000
    },
    {
      "epoch": 0.13154747668022004,
      "grad_norm": 9.95832347869873,
      "learning_rate": 1.947381009327912e-05,
      "loss": 0.7817,
      "step": 1100
    },
    {
      "epoch": 0.1435063381966037,
      "grad_norm": 7.512314319610596,
      "learning_rate": 1.9425974647213588e-05,
      "loss": 0.7353,
      "step": 1200
    },
    {
      "epoch": 0.15546519971298733,
      "grad_norm": 8.9541597366333,
      "learning_rate": 1.937813920114805e-05,
      "loss": 0.7289,
      "step": 1300
    },
    {
      "epoch": 0.16742406122937095,
      "grad_norm": 6.535177230834961,
      "learning_rate": 1.933030375508252e-05,
      "loss": 0.7066,
      "step": 1400
    },
    {
      "epoch": 0.1793829227457546,
      "grad_norm": 2.1675820350646973,
      "learning_rate": 1.9282468309016985e-05,
      "loss": 0.6482,
      "step": 1500
    },
    {
      "epoch": 0.19134178426213824,
      "grad_norm": 10.124380111694336,
      "learning_rate": 1.923463286295145e-05,
      "loss": 0.7834,
      "step": 1600
    },
    {
      "epoch": 0.2033006457785219,
      "grad_norm": 11.924543380737305,
      "learning_rate": 1.9186797416885912e-05,
      "loss": 0.7899,
      "step": 1700
    },
    {
      "epoch": 0.21525950729490553,
      "grad_norm": 8.078987121582031,
      "learning_rate": 1.913896197082038e-05,
      "loss": 0.7553,
      "step": 1800
    },
    {
      "epoch": 0.22721836881128918,
      "grad_norm": 8.449869155883789,
      "learning_rate": 1.9091126524754843e-05,
      "loss": 0.7453,
      "step": 1900
    },
    {
      "epoch": 0.2391772303276728,
      "grad_norm": 5.966902732849121,
      "learning_rate": 1.904329107868931e-05,
      "loss": 0.8052,
      "step": 2000
    },
    {
      "epoch": 0.25113609184405644,
      "grad_norm": 14.897256851196289,
      "learning_rate": 1.8995455632623777e-05,
      "loss": 0.6477,
      "step": 2100
    },
    {
      "epoch": 0.2630949533604401,
      "grad_norm": 7.84080696105957,
      "learning_rate": 1.8947620186558244e-05,
      "loss": 0.6636,
      "step": 2200
    },
    {
      "epoch": 0.27505381487682373,
      "grad_norm": 192.8009796142578,
      "learning_rate": 1.8899784740492707e-05,
      "loss": 0.7251,
      "step": 2300
    },
    {
      "epoch": 0.2870126763932074,
      "grad_norm": 10.211992263793945,
      "learning_rate": 1.885194929442717e-05,
      "loss": 0.7107,
      "step": 2400
    },
    {
      "epoch": 0.298971537909591,
      "grad_norm": 9.302628517150879,
      "learning_rate": 1.8804113848361638e-05,
      "loss": 0.7254,
      "step": 2500
    },
    {
      "epoch": 0.31093039942597467,
      "grad_norm": 7.219296455383301,
      "learning_rate": 1.87562784022961e-05,
      "loss": 0.6874,
      "step": 2600
    },
    {
      "epoch": 0.3228892609423583,
      "grad_norm": 6.387142658233643,
      "learning_rate": 1.8708442956230568e-05,
      "loss": 0.6996,
      "step": 2700
    },
    {
      "epoch": 0.3348481224587419,
      "grad_norm": 7.456765174865723,
      "learning_rate": 1.8660607510165035e-05,
      "loss": 0.6808,
      "step": 2800
    },
    {
      "epoch": 0.34680698397512555,
      "grad_norm": 8.012441635131836,
      "learning_rate": 1.86127720640995e-05,
      "loss": 0.7404,
      "step": 2900
    },
    {
      "epoch": 0.3587658454915092,
      "grad_norm": 13.470535278320312,
      "learning_rate": 1.8564936618033966e-05,
      "loss": 0.6565,
      "step": 3000
    },
    {
      "epoch": 0.37072470700789284,
      "grad_norm": 3.2570741176605225,
      "learning_rate": 1.851710117196843e-05,
      "loss": 0.628,
      "step": 3100
    },
    {
      "epoch": 0.3826835685242765,
      "grad_norm": 10.644498825073242,
      "learning_rate": 1.8469265725902896e-05,
      "loss": 0.6526,
      "step": 3200
    },
    {
      "epoch": 0.39464243004066013,
      "grad_norm": 7.0747480392456055,
      "learning_rate": 1.842143027983736e-05,
      "loss": 0.6916,
      "step": 3300
    },
    {
      "epoch": 0.4066012915570438,
      "grad_norm": 10.368650436401367,
      "learning_rate": 1.8373594833771827e-05,
      "loss": 0.7668,
      "step": 3400
    },
    {
      "epoch": 0.4185601530734274,
      "grad_norm": 8.98692798614502,
      "learning_rate": 1.8325759387706293e-05,
      "loss": 0.6355,
      "step": 3500
    },
    {
      "epoch": 0.43051901458981107,
      "grad_norm": 10.5388765335083,
      "learning_rate": 1.8277923941640757e-05,
      "loss": 0.6845,
      "step": 3600
    },
    {
      "epoch": 0.4424778761061947,
      "grad_norm": 12.547924995422363,
      "learning_rate": 1.823008849557522e-05,
      "loss": 0.6306,
      "step": 3700
    },
    {
      "epoch": 0.45443673762257836,
      "grad_norm": 17.434024810791016,
      "learning_rate": 1.8182253049509687e-05,
      "loss": 0.664,
      "step": 3800
    },
    {
      "epoch": 0.46639559913896195,
      "grad_norm": 10.674860000610352,
      "learning_rate": 1.8134417603444154e-05,
      "loss": 0.7647,
      "step": 3900
    },
    {
      "epoch": 0.4783544606553456,
      "grad_norm": 4.601812839508057,
      "learning_rate": 1.8086582157378618e-05,
      "loss": 0.7262,
      "step": 4000
    },
    {
      "epoch": 0.49031332217172924,
      "grad_norm": 7.633907318115234,
      "learning_rate": 1.8038746711313085e-05,
      "loss": 0.6705,
      "step": 4100
    },
    {
      "epoch": 0.5022721836881129,
      "grad_norm": 5.027950763702393,
      "learning_rate": 1.7990911265247552e-05,
      "loss": 0.6929,
      "step": 4200
    },
    {
      "epoch": 0.5142310452044966,
      "grad_norm": 7.682476997375488,
      "learning_rate": 1.7943075819182015e-05,
      "loss": 0.5985,
      "step": 4300
    },
    {
      "epoch": 0.5261899067208802,
      "grad_norm": 6.129551887512207,
      "learning_rate": 1.789524037311648e-05,
      "loss": 0.6506,
      "step": 4400
    },
    {
      "epoch": 0.5381487682372638,
      "grad_norm": 6.67578649520874,
      "learning_rate": 1.7847404927050946e-05,
      "loss": 0.6636,
      "step": 4500
    },
    {
      "epoch": 0.5501076297536475,
      "grad_norm": 4.884012222290039,
      "learning_rate": 1.7799569480985413e-05,
      "loss": 0.7869,
      "step": 4600
    },
    {
      "epoch": 0.562066491270031,
      "grad_norm": 15.302020072937012,
      "learning_rate": 1.7751734034919876e-05,
      "loss": 0.6876,
      "step": 4700
    },
    {
      "epoch": 0.5740253527864148,
      "grad_norm": 3.3374056816101074,
      "learning_rate": 1.7703898588854343e-05,
      "loss": 0.6504,
      "step": 4800
    },
    {
      "epoch": 0.5859842143027983,
      "grad_norm": 12.448984146118164,
      "learning_rate": 1.765606314278881e-05,
      "loss": 0.673,
      "step": 4900
    },
    {
      "epoch": 0.597943075819182,
      "grad_norm": 6.910583972930908,
      "learning_rate": 1.7608227696723274e-05,
      "loss": 0.6487,
      "step": 5000
    },
    {
      "epoch": 0.6099019373355656,
      "grad_norm": 5.234573841094971,
      "learning_rate": 1.7560392250657737e-05,
      "loss": 0.6494,
      "step": 5100
    },
    {
      "epoch": 0.6218607988519493,
      "grad_norm": 7.9048333168029785,
      "learning_rate": 1.7512556804592204e-05,
      "loss": 0.7375,
      "step": 5200
    },
    {
      "epoch": 0.6338196603683329,
      "grad_norm": 3.8897156715393066,
      "learning_rate": 1.746472135852667e-05,
      "loss": 0.6996,
      "step": 5300
    },
    {
      "epoch": 0.6457785218847166,
      "grad_norm": 0.877735435962677,
      "learning_rate": 1.7416885912461135e-05,
      "loss": 0.655,
      "step": 5400
    },
    {
      "epoch": 0.6577373834011002,
      "grad_norm": 11.931236267089844,
      "learning_rate": 1.73690504663956e-05,
      "loss": 0.6442,
      "step": 5500
    },
    {
      "epoch": 0.6696962449174838,
      "grad_norm": 8.28175163269043,
      "learning_rate": 1.7321215020330065e-05,
      "loss": 0.6795,
      "step": 5600
    },
    {
      "epoch": 0.6816551064338675,
      "grad_norm": 5.503127574920654,
      "learning_rate": 1.7273379574264532e-05,
      "loss": 0.7165,
      "step": 5700
    },
    {
      "epoch": 0.6936139679502511,
      "grad_norm": 11.059713363647461,
      "learning_rate": 1.7225544128198996e-05,
      "loss": 0.7041,
      "step": 5800
    },
    {
      "epoch": 0.7055728294666348,
      "grad_norm": 5.214011192321777,
      "learning_rate": 1.7177708682133463e-05,
      "loss": 0.7242,
      "step": 5900
    },
    {
      "epoch": 0.7175316909830184,
      "grad_norm": 4.18586540222168,
      "learning_rate": 1.712987323606793e-05,
      "loss": 0.7379,
      "step": 6000
    },
    {
      "epoch": 0.7294905524994021,
      "grad_norm": 16.325096130371094,
      "learning_rate": 1.7082037790002393e-05,
      "loss": 0.6776,
      "step": 6100
    },
    {
      "epoch": 0.7414494140157857,
      "grad_norm": 7.303318977355957,
      "learning_rate": 1.703420234393686e-05,
      "loss": 0.6482,
      "step": 6200
    },
    {
      "epoch": 0.7534082755321694,
      "grad_norm": 5.563650608062744,
      "learning_rate": 1.6986366897871324e-05,
      "loss": 0.6782,
      "step": 6300
    },
    {
      "epoch": 0.765367137048553,
      "grad_norm": 9.927305221557617,
      "learning_rate": 1.6938531451805787e-05,
      "loss": 0.6639,
      "step": 6400
    },
    {
      "epoch": 0.7773259985649367,
      "grad_norm": 5.329163074493408,
      "learning_rate": 1.6890696005740254e-05,
      "loss": 0.6943,
      "step": 6500
    },
    {
      "epoch": 0.7892848600813203,
      "grad_norm": 3.4567723274230957,
      "learning_rate": 1.684286055967472e-05,
      "loss": 0.682,
      "step": 6600
    },
    {
      "epoch": 0.8012437215977039,
      "grad_norm": 12.245083808898926,
      "learning_rate": 1.6795025113609188e-05,
      "loss": 0.6462,
      "step": 6700
    },
    {
      "epoch": 0.8132025831140876,
      "grad_norm": 6.390713214874268,
      "learning_rate": 1.674718966754365e-05,
      "loss": 0.6404,
      "step": 6800
    },
    {
      "epoch": 0.8251614446304711,
      "grad_norm": 15.113524436950684,
      "learning_rate": 1.669935422147812e-05,
      "loss": 0.6438,
      "step": 6900
    },
    {
      "epoch": 0.8371203061468548,
      "grad_norm": 5.212280750274658,
      "learning_rate": 1.6651518775412582e-05,
      "loss": 0.6993,
      "step": 7000
    },
    {
      "epoch": 0.8490791676632384,
      "grad_norm": 6.735912322998047,
      "learning_rate": 1.6603683329347045e-05,
      "loss": 0.6542,
      "step": 7100
    },
    {
      "epoch": 0.8610380291796221,
      "grad_norm": 8.5941801071167,
      "learning_rate": 1.6555847883281512e-05,
      "loss": 0.6963,
      "step": 7200
    },
    {
      "epoch": 0.8729968906960057,
      "grad_norm": 5.308830738067627,
      "learning_rate": 1.650801243721598e-05,
      "loss": 0.679,
      "step": 7300
    },
    {
      "epoch": 0.8849557522123894,
      "grad_norm": 7.844737529754639,
      "learning_rate": 1.6460176991150443e-05,
      "loss": 0.6558,
      "step": 7400
    },
    {
      "epoch": 0.896914613728773,
      "grad_norm": 12.089272499084473,
      "learning_rate": 1.641234154508491e-05,
      "loss": 0.7164,
      "step": 7500
    },
    {
      "epoch": 0.9088734752451567,
      "grad_norm": 9.289371490478516,
      "learning_rate": 1.6364506099019373e-05,
      "loss": 0.6176,
      "step": 7600
    },
    {
      "epoch": 0.9208323367615403,
      "grad_norm": 4.625111103057861,
      "learning_rate": 1.631667065295384e-05,
      "loss": 0.5448,
      "step": 7700
    },
    {
      "epoch": 0.9327911982779239,
      "grad_norm": 4.052725791931152,
      "learning_rate": 1.6268835206888304e-05,
      "loss": 0.683,
      "step": 7800
    },
    {
      "epoch": 0.9447500597943076,
      "grad_norm": 2.4190585613250732,
      "learning_rate": 1.622099976082277e-05,
      "loss": 0.5898,
      "step": 7900
    },
    {
      "epoch": 0.9567089213106912,
      "grad_norm": 6.39670467376709,
      "learning_rate": 1.6173164314757238e-05,
      "loss": 0.6589,
      "step": 8000
    },
    {
      "epoch": 0.9686677828270749,
      "grad_norm": 11.293486595153809,
      "learning_rate": 1.61253288686917e-05,
      "loss": 0.7016,
      "step": 8100
    },
    {
      "epoch": 0.9806266443434585,
      "grad_norm": 5.604856014251709,
      "learning_rate": 1.6077493422626168e-05,
      "loss": 0.5861,
      "step": 8200
    },
    {
      "epoch": 0.9925855058598422,
      "grad_norm": 6.713145732879639,
      "learning_rate": 1.602965797656063e-05,
      "loss": 0.6557,
      "step": 8300
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8129017790402153,
      "eval_f1_macro": 0.812039124462268,
      "eval_loss": 0.5573570728302002,
      "eval_runtime": 109.3535,
      "eval_samples_per_second": 122.337,
      "eval_steps_per_second": 7.654,
      "step": 8362
    },
    {
      "epoch": 1.0045443673762258,
      "grad_norm": 5.154976844787598,
      "learning_rate": 1.59818225304951e-05,
      "loss": 0.6142,
      "step": 8400
    },
    {
      "epoch": 1.0165032288926095,
      "grad_norm": 13.178009033203125,
      "learning_rate": 1.5933987084429562e-05,
      "loss": 0.5557,
      "step": 8500
    },
    {
      "epoch": 1.0284620904089932,
      "grad_norm": 3.2897722721099854,
      "learning_rate": 1.588615163836403e-05,
      "loss": 0.5364,
      "step": 8600
    },
    {
      "epoch": 1.0404209519253766,
      "grad_norm": 10.189860343933105,
      "learning_rate": 1.5838316192298496e-05,
      "loss": 0.6369,
      "step": 8700
    },
    {
      "epoch": 1.0523798134417603,
      "grad_norm": 6.784334182739258,
      "learning_rate": 1.579048074623296e-05,
      "loss": 0.6165,
      "step": 8800
    },
    {
      "epoch": 1.064338674958144,
      "grad_norm": 4.947251319885254,
      "learning_rate": 1.5742645300167426e-05,
      "loss": 0.6043,
      "step": 8900
    },
    {
      "epoch": 1.0762975364745275,
      "grad_norm": 6.066694259643555,
      "learning_rate": 1.569480985410189e-05,
      "loss": 0.5534,
      "step": 9000
    },
    {
      "epoch": 1.0882563979909112,
      "grad_norm": 4.157088756561279,
      "learning_rate": 1.5646974408036357e-05,
      "loss": 0.6178,
      "step": 9100
    },
    {
      "epoch": 1.100215259507295,
      "grad_norm": 6.079089641571045,
      "learning_rate": 1.559913896197082e-05,
      "loss": 0.5762,
      "step": 9200
    },
    {
      "epoch": 1.1121741210236786,
      "grad_norm": 9.00609302520752,
      "learning_rate": 1.5551303515905287e-05,
      "loss": 0.556,
      "step": 9300
    },
    {
      "epoch": 1.124132982540062,
      "grad_norm": 5.065130710601807,
      "learning_rate": 1.5503468069839754e-05,
      "loss": 0.5763,
      "step": 9400
    },
    {
      "epoch": 1.1360918440564458,
      "grad_norm": 6.66310977935791,
      "learning_rate": 1.5455632623774218e-05,
      "loss": 0.5775,
      "step": 9500
    },
    {
      "epoch": 1.1480507055728295,
      "grad_norm": 9.67817211151123,
      "learning_rate": 1.5407797177708685e-05,
      "loss": 0.5664,
      "step": 9600
    },
    {
      "epoch": 1.160009567089213,
      "grad_norm": 1.7944118976593018,
      "learning_rate": 1.535996173164315e-05,
      "loss": 0.5904,
      "step": 9700
    },
    {
      "epoch": 1.1719684286055967,
      "grad_norm": 7.081860542297363,
      "learning_rate": 1.5312126285577612e-05,
      "loss": 0.5174,
      "step": 9800
    },
    {
      "epoch": 1.1839272901219804,
      "grad_norm": 13.359214782714844,
      "learning_rate": 1.526429083951208e-05,
      "loss": 0.5496,
      "step": 9900
    },
    {
      "epoch": 1.195886151638364,
      "grad_norm": 3.712728500366211,
      "learning_rate": 1.5216455393446546e-05,
      "loss": 0.6033,
      "step": 10000
    },
    {
      "epoch": 1.2078450131547476,
      "grad_norm": 19.356027603149414,
      "learning_rate": 1.5168619947381011e-05,
      "loss": 0.5771,
      "step": 10100
    },
    {
      "epoch": 1.2198038746711313,
      "grad_norm": 12.235816955566406,
      "learning_rate": 1.5120784501315475e-05,
      "loss": 0.614,
      "step": 10200
    },
    {
      "epoch": 1.231762736187515,
      "grad_norm": 11.030449867248535,
      "learning_rate": 1.5072949055249942e-05,
      "loss": 0.5624,
      "step": 10300
    },
    {
      "epoch": 1.2437215977038987,
      "grad_norm": 10.653669357299805,
      "learning_rate": 1.5025113609184407e-05,
      "loss": 0.5993,
      "step": 10400
    },
    {
      "epoch": 1.2556804592202822,
      "grad_norm": 10.220147132873535,
      "learning_rate": 1.4977278163118872e-05,
      "loss": 0.5609,
      "step": 10500
    },
    {
      "epoch": 1.2676393207366659,
      "grad_norm": 7.640522480010986,
      "learning_rate": 1.4929442717053337e-05,
      "loss": 0.5152,
      "step": 10600
    },
    {
      "epoch": 1.2795981822530496,
      "grad_norm": 2.249002456665039,
      "learning_rate": 1.4881607270987804e-05,
      "loss": 0.6559,
      "step": 10700
    },
    {
      "epoch": 1.291557043769433,
      "grad_norm": 5.191122055053711,
      "learning_rate": 1.483377182492227e-05,
      "loss": 0.5611,
      "step": 10800
    },
    {
      "epoch": 1.3035159052858167,
      "grad_norm": 7.298190116882324,
      "learning_rate": 1.4785936378856733e-05,
      "loss": 0.5586,
      "step": 10900
    },
    {
      "epoch": 1.3154747668022004,
      "grad_norm": 5.303060054779053,
      "learning_rate": 1.47381009327912e-05,
      "loss": 0.5592,
      "step": 11000
    },
    {
      "epoch": 1.3274336283185841,
      "grad_norm": 14.981128692626953,
      "learning_rate": 1.4690265486725665e-05,
      "loss": 0.5238,
      "step": 11100
    },
    {
      "epoch": 1.3393924898349678,
      "grad_norm": 8.794685363769531,
      "learning_rate": 1.4642430040660129e-05,
      "loss": 0.5257,
      "step": 11200
    },
    {
      "epoch": 1.3513513513513513,
      "grad_norm": 7.4744343757629395,
      "learning_rate": 1.4594594594594596e-05,
      "loss": 0.5422,
      "step": 11300
    },
    {
      "epoch": 1.363310212867735,
      "grad_norm": 9.148530960083008,
      "learning_rate": 1.454675914852906e-05,
      "loss": 0.6078,
      "step": 11400
    },
    {
      "epoch": 1.3752690743841187,
      "grad_norm": 2.2321672439575195,
      "learning_rate": 1.4498923702463528e-05,
      "loss": 0.5953,
      "step": 11500
    },
    {
      "epoch": 1.3872279359005022,
      "grad_norm": 5.412552833557129,
      "learning_rate": 1.4451088256397991e-05,
      "loss": 0.5939,
      "step": 11600
    },
    {
      "epoch": 1.399186797416886,
      "grad_norm": 12.952445030212402,
      "learning_rate": 1.4403252810332458e-05,
      "loss": 0.5768,
      "step": 11700
    },
    {
      "epoch": 1.4111456589332696,
      "grad_norm": 5.152866363525391,
      "learning_rate": 1.4355417364266923e-05,
      "loss": 0.6029,
      "step": 11800
    },
    {
      "epoch": 1.423104520449653,
      "grad_norm": 21.869892120361328,
      "learning_rate": 1.4307581918201387e-05,
      "loss": 0.5664,
      "step": 11900
    },
    {
      "epoch": 1.4350633819660368,
      "grad_norm": 3.4350805282592773,
      "learning_rate": 1.4259746472135854e-05,
      "loss": 0.567,
      "step": 12000
    },
    {
      "epoch": 1.4470222434824205,
      "grad_norm": 8.412625312805176,
      "learning_rate": 1.421191102607032e-05,
      "loss": 0.5808,
      "step": 12100
    },
    {
      "epoch": 1.4589811049988042,
      "grad_norm": 10.596674919128418,
      "learning_rate": 1.4164075580004786e-05,
      "loss": 0.6122,
      "step": 12200
    },
    {
      "epoch": 1.4709399665151879,
      "grad_norm": 10.628725051879883,
      "learning_rate": 1.411624013393925e-05,
      "loss": 0.5588,
      "step": 12300
    },
    {
      "epoch": 1.4828988280315714,
      "grad_norm": 7.755724906921387,
      "learning_rate": 1.4068404687873715e-05,
      "loss": 0.6083,
      "step": 12400
    },
    {
      "epoch": 1.494857689547955,
      "grad_norm": 11.871667861938477,
      "learning_rate": 1.4020569241808182e-05,
      "loss": 0.577,
      "step": 12500
    },
    {
      "epoch": 1.5068165510643388,
      "grad_norm": 7.434650421142578,
      "learning_rate": 1.3972733795742645e-05,
      "loss": 0.5209,
      "step": 12600
    },
    {
      "epoch": 1.5187754125807222,
      "grad_norm": 11.013934135437012,
      "learning_rate": 1.3924898349677112e-05,
      "loss": 0.5538,
      "step": 12700
    },
    {
      "epoch": 1.530734274097106,
      "grad_norm": 5.856832027435303,
      "learning_rate": 1.3877062903611578e-05,
      "loss": 0.5356,
      "step": 12800
    },
    {
      "epoch": 1.5426931356134896,
      "grad_norm": 1.5914959907531738,
      "learning_rate": 1.3829227457546041e-05,
      "loss": 0.5511,
      "step": 12900
    },
    {
      "epoch": 1.5546519971298731,
      "grad_norm": 9.322961807250977,
      "learning_rate": 1.3781392011480508e-05,
      "loss": 0.6215,
      "step": 13000
    },
    {
      "epoch": 1.5666108586462568,
      "grad_norm": 6.530704975128174,
      "learning_rate": 1.3733556565414973e-05,
      "loss": 0.5838,
      "step": 13100
    },
    {
      "epoch": 1.5785697201626405,
      "grad_norm": 4.334164142608643,
      "learning_rate": 1.368572111934944e-05,
      "loss": 0.5624,
      "step": 13200
    },
    {
      "epoch": 1.590528581679024,
      "grad_norm": 4.356767177581787,
      "learning_rate": 1.3637885673283904e-05,
      "loss": 0.5207,
      "step": 13300
    },
    {
      "epoch": 1.602487443195408,
      "grad_norm": 9.816802978515625,
      "learning_rate": 1.359005022721837e-05,
      "loss": 0.5362,
      "step": 13400
    },
    {
      "epoch": 1.6144463047117914,
      "grad_norm": 14.8916015625,
      "learning_rate": 1.3542214781152836e-05,
      "loss": 0.5766,
      "step": 13500
    },
    {
      "epoch": 1.626405166228175,
      "grad_norm": 7.607301712036133,
      "learning_rate": 1.34943793350873e-05,
      "loss": 0.5816,
      "step": 13600
    },
    {
      "epoch": 1.6383640277445588,
      "grad_norm": 5.370174407958984,
      "learning_rate": 1.3446543889021766e-05,
      "loss": 0.6298,
      "step": 13700
    },
    {
      "epoch": 1.6503228892609423,
      "grad_norm": 1.880420207977295,
      "learning_rate": 1.3398708442956232e-05,
      "loss": 0.5395,
      "step": 13800
    },
    {
      "epoch": 1.662281750777326,
      "grad_norm": 10.256560325622559,
      "learning_rate": 1.3350872996890699e-05,
      "loss": 0.5053,
      "step": 13900
    },
    {
      "epoch": 1.6742406122937097,
      "grad_norm": 3.7937235832214355,
      "learning_rate": 1.3303037550825162e-05,
      "loss": 0.5001,
      "step": 14000
    },
    {
      "epoch": 1.6861994738100932,
      "grad_norm": 10.07287311553955,
      "learning_rate": 1.3255202104759627e-05,
      "loss": 0.5522,
      "step": 14100
    },
    {
      "epoch": 1.6981583353264769,
      "grad_norm": 4.8769121170043945,
      "learning_rate": 1.3207366658694094e-05,
      "loss": 0.5921,
      "step": 14200
    },
    {
      "epoch": 1.7101171968428606,
      "grad_norm": 5.248197078704834,
      "learning_rate": 1.3159531212628558e-05,
      "loss": 0.5923,
      "step": 14300
    },
    {
      "epoch": 1.722076058359244,
      "grad_norm": 7.625607013702393,
      "learning_rate": 1.3111695766563025e-05,
      "loss": 0.5815,
      "step": 14400
    },
    {
      "epoch": 1.734034919875628,
      "grad_norm": 2.49535870552063,
      "learning_rate": 1.306386032049749e-05,
      "loss": 0.5428,
      "step": 14500
    },
    {
      "epoch": 1.7459937813920114,
      "grad_norm": 11.239043235778809,
      "learning_rate": 1.3016024874431957e-05,
      "loss": 0.5589,
      "step": 14600
    },
    {
      "epoch": 1.7579526429083951,
      "grad_norm": 4.03919792175293,
      "learning_rate": 1.296818942836642e-05,
      "loss": 0.5776,
      "step": 14700
    },
    {
      "epoch": 1.7699115044247788,
      "grad_norm": 5.946163654327393,
      "learning_rate": 1.2920353982300886e-05,
      "loss": 0.6422,
      "step": 14800
    },
    {
      "epoch": 1.7818703659411623,
      "grad_norm": 6.106709957122803,
      "learning_rate": 1.2872518536235353e-05,
      "loss": 0.5842,
      "step": 14900
    },
    {
      "epoch": 1.793829227457546,
      "grad_norm": 3.8036794662475586,
      "learning_rate": 1.2824683090169816e-05,
      "loss": 0.5948,
      "step": 15000
    },
    {
      "epoch": 1.8057880889739297,
      "grad_norm": 6.08643913269043,
      "learning_rate": 1.2776847644104281e-05,
      "loss": 0.5891,
      "step": 15100
    },
    {
      "epoch": 1.8177469504903132,
      "grad_norm": 8.885833740234375,
      "learning_rate": 1.2729012198038748e-05,
      "loss": 0.5414,
      "step": 15200
    },
    {
      "epoch": 1.829705812006697,
      "grad_norm": 12.604742050170898,
      "learning_rate": 1.2681176751973214e-05,
      "loss": 0.4884,
      "step": 15300
    },
    {
      "epoch": 1.8416646735230806,
      "grad_norm": 12.876761436462402,
      "learning_rate": 1.2633341305907679e-05,
      "loss": 0.5224,
      "step": 15400
    },
    {
      "epoch": 1.853623535039464,
      "grad_norm": 13.316682815551758,
      "learning_rate": 1.2585505859842144e-05,
      "loss": 0.6101,
      "step": 15500
    },
    {
      "epoch": 1.865582396555848,
      "grad_norm": 3.606099843978882,
      "learning_rate": 1.2537670413776611e-05,
      "loss": 0.5707,
      "step": 15600
    },
    {
      "epoch": 1.8775412580722315,
      "grad_norm": 8.87705135345459,
      "learning_rate": 1.2489834967711075e-05,
      "loss": 0.6296,
      "step": 15700
    },
    {
      "epoch": 1.8895001195886152,
      "grad_norm": 11.461599349975586,
      "learning_rate": 1.244199952164554e-05,
      "loss": 0.5824,
      "step": 15800
    },
    {
      "epoch": 1.901458981104999,
      "grad_norm": 10.488778114318848,
      "learning_rate": 1.2394164075580007e-05,
      "loss": 0.5853,
      "step": 15900
    },
    {
      "epoch": 1.9134178426213824,
      "grad_norm": 9.18690299987793,
      "learning_rate": 1.234632862951447e-05,
      "loss": 0.5555,
      "step": 16000
    },
    {
      "epoch": 1.925376704137766,
      "grad_norm": 7.993716239929199,
      "learning_rate": 1.2298493183448936e-05,
      "loss": 0.6018,
      "step": 16100
    },
    {
      "epoch": 1.9373355656541498,
      "grad_norm": 5.160972595214844,
      "learning_rate": 1.2250657737383402e-05,
      "loss": 0.5523,
      "step": 16200
    },
    {
      "epoch": 1.9492944271705333,
      "grad_norm": 3.271862745285034,
      "learning_rate": 1.2202822291317868e-05,
      "loss": 0.5744,
      "step": 16300
    },
    {
      "epoch": 1.961253288686917,
      "grad_norm": 7.746436595916748,
      "learning_rate": 1.2154986845252333e-05,
      "loss": 0.5761,
      "step": 16400
    },
    {
      "epoch": 1.9732121502033007,
      "grad_norm": 6.884366512298584,
      "learning_rate": 1.2107151399186798e-05,
      "loss": 0.5415,
      "step": 16500
    },
    {
      "epoch": 1.9851710117196841,
      "grad_norm": 7.507559299468994,
      "learning_rate": 1.2059315953121265e-05,
      "loss": 0.5972,
      "step": 16600
    },
    {
      "epoch": 1.997129873236068,
      "grad_norm": 7.407329082489014,
      "learning_rate": 1.2011480507055729e-05,
      "loss": 0.5585,
      "step": 16700
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.851472566900882,
      "eval_f1_macro": 0.8479233844379933,
      "eval_loss": 0.43658870458602905,
      "eval_runtime": 120.8922,
      "eval_samples_per_second": 110.661,
      "eval_steps_per_second": 6.924,
      "step": 16724
    },
    {
      "epoch": 2.0090887347524515,
      "grad_norm": 3.210178852081299,
      "learning_rate": 1.1963645060990194e-05,
      "loss": 0.5337,
      "step": 16800
    },
    {
      "epoch": 2.021047596268835,
      "grad_norm": 26.939672470092773,
      "learning_rate": 1.191580961492466e-05,
      "loss": 0.4634,
      "step": 16900
    },
    {
      "epoch": 2.033006457785219,
      "grad_norm": 13.998637199401855,
      "learning_rate": 1.1867974168859126e-05,
      "loss": 0.4753,
      "step": 17000
    },
    {
      "epoch": 2.0449653193016024,
      "grad_norm": 7.663667678833008,
      "learning_rate": 1.182013872279359e-05,
      "loss": 0.4866,
      "step": 17100
    },
    {
      "epoch": 2.0569241808179863,
      "grad_norm": 15.890419960021973,
      "learning_rate": 1.1772303276728057e-05,
      "loss": 0.4537,
      "step": 17200
    },
    {
      "epoch": 2.06888304233437,
      "grad_norm": 9.144795417785645,
      "learning_rate": 1.1724467830662522e-05,
      "loss": 0.4758,
      "step": 17300
    },
    {
      "epoch": 2.0808419038507533,
      "grad_norm": 4.668581485748291,
      "learning_rate": 1.1676632384596987e-05,
      "loss": 0.4601,
      "step": 17400
    },
    {
      "epoch": 2.092800765367137,
      "grad_norm": 4.8298444747924805,
      "learning_rate": 1.1628796938531452e-05,
      "loss": 0.4572,
      "step": 17500
    },
    {
      "epoch": 2.1047596268835207,
      "grad_norm": 18.436725616455078,
      "learning_rate": 1.158096149246592e-05,
      "loss": 0.4677,
      "step": 17600
    },
    {
      "epoch": 2.116718488399904,
      "grad_norm": 2.7953054904937744,
      "learning_rate": 1.1533126046400384e-05,
      "loss": 0.4602,
      "step": 17700
    },
    {
      "epoch": 2.128677349916288,
      "grad_norm": 39.37583923339844,
      "learning_rate": 1.1485290600334848e-05,
      "loss": 0.5205,
      "step": 17800
    },
    {
      "epoch": 2.1406362114326716,
      "grad_norm": 8.586292266845703,
      "learning_rate": 1.1437455154269315e-05,
      "loss": 0.4874,
      "step": 17900
    },
    {
      "epoch": 2.152595072949055,
      "grad_norm": 5.811395168304443,
      "learning_rate": 1.138961970820378e-05,
      "loss": 0.4794,
      "step": 18000
    },
    {
      "epoch": 2.164553934465439,
      "grad_norm": 12.002433776855469,
      "learning_rate": 1.1341784262138245e-05,
      "loss": 0.5032,
      "step": 18100
    },
    {
      "epoch": 2.1765127959818225,
      "grad_norm": 6.843281269073486,
      "learning_rate": 1.129394881607271e-05,
      "loss": 0.4953,
      "step": 18200
    },
    {
      "epoch": 2.188471657498206,
      "grad_norm": 4.64076042175293,
      "learning_rate": 1.1246113370007178e-05,
      "loss": 0.4729,
      "step": 18300
    },
    {
      "epoch": 2.20043051901459,
      "grad_norm": 10.994033813476562,
      "learning_rate": 1.1198277923941641e-05,
      "loss": 0.4816,
      "step": 18400
    },
    {
      "epoch": 2.2123893805309733,
      "grad_norm": 8.366049766540527,
      "learning_rate": 1.1150442477876106e-05,
      "loss": 0.5261,
      "step": 18500
    },
    {
      "epoch": 2.2243482420473573,
      "grad_norm": 8.339922904968262,
      "learning_rate": 1.1102607031810573e-05,
      "loss": 0.4798,
      "step": 18600
    },
    {
      "epoch": 2.2363071035637407,
      "grad_norm": 14.27596378326416,
      "learning_rate": 1.1054771585745038e-05,
      "loss": 0.4655,
      "step": 18700
    },
    {
      "epoch": 2.248265965080124,
      "grad_norm": 9.970073699951172,
      "learning_rate": 1.1006936139679502e-05,
      "loss": 0.4819,
      "step": 18800
    },
    {
      "epoch": 2.260224826596508,
      "grad_norm": 3.6476752758026123,
      "learning_rate": 1.0959100693613969e-05,
      "loss": 0.497,
      "step": 18900
    },
    {
      "epoch": 2.2721836881128916,
      "grad_norm": 14.71412181854248,
      "learning_rate": 1.0911265247548434e-05,
      "loss": 0.4491,
      "step": 19000
    },
    {
      "epoch": 2.2841425496292755,
      "grad_norm": 8.654979705810547,
      "learning_rate": 1.08634298014829e-05,
      "loss": 0.4901,
      "step": 19100
    },
    {
      "epoch": 2.296101411145659,
      "grad_norm": 10.811751365661621,
      "learning_rate": 1.0815594355417365e-05,
      "loss": 0.4415,
      "step": 19200
    },
    {
      "epoch": 2.3080602726620425,
      "grad_norm": 20.0969295501709,
      "learning_rate": 1.0767758909351832e-05,
      "loss": 0.4337,
      "step": 19300
    },
    {
      "epoch": 2.320019134178426,
      "grad_norm": 8.441801071166992,
      "learning_rate": 1.0719923463286297e-05,
      "loss": 0.5005,
      "step": 19400
    },
    {
      "epoch": 2.33197799569481,
      "grad_norm": 3.7322146892547607,
      "learning_rate": 1.067208801722076e-05,
      "loss": 0.4226,
      "step": 19500
    },
    {
      "epoch": 2.3439368572111934,
      "grad_norm": 1.0483386516571045,
      "learning_rate": 1.0624252571155227e-05,
      "loss": 0.4732,
      "step": 19600
    },
    {
      "epoch": 2.3558957187275773,
      "grad_norm": 3.6026294231414795,
      "learning_rate": 1.0576417125089693e-05,
      "loss": 0.5147,
      "step": 19700
    },
    {
      "epoch": 2.367854580243961,
      "grad_norm": 9.04989242553711,
      "learning_rate": 1.0528581679024156e-05,
      "loss": 0.4601,
      "step": 19800
    },
    {
      "epoch": 2.3798134417603443,
      "grad_norm": 15.527580261230469,
      "learning_rate": 1.0480746232958623e-05,
      "loss": 0.4744,
      "step": 19900
    },
    {
      "epoch": 2.391772303276728,
      "grad_norm": 7.995116233825684,
      "learning_rate": 1.0432910786893088e-05,
      "loss": 0.4595,
      "step": 20000
    },
    {
      "epoch": 2.4037311647931117,
      "grad_norm": 9.750601768493652,
      "learning_rate": 1.0385075340827555e-05,
      "loss": 0.5024,
      "step": 20100
    },
    {
      "epoch": 2.415690026309495,
      "grad_norm": 0.29755330085754395,
      "learning_rate": 1.0337239894762019e-05,
      "loss": 0.4906,
      "step": 20200
    },
    {
      "epoch": 2.427648887825879,
      "grad_norm": 10.195441246032715,
      "learning_rate": 1.0289404448696486e-05,
      "loss": 0.5158,
      "step": 20300
    },
    {
      "epoch": 2.4396077493422625,
      "grad_norm": 12.702961921691895,
      "learning_rate": 1.0241569002630951e-05,
      "loss": 0.5291,
      "step": 20400
    },
    {
      "epoch": 2.451566610858646,
      "grad_norm": 0.2969897985458374,
      "learning_rate": 1.0193733556565414e-05,
      "loss": 0.446,
      "step": 20500
    },
    {
      "epoch": 2.46352547237503,
      "grad_norm": 2.109304666519165,
      "learning_rate": 1.0145898110499881e-05,
      "loss": 0.4562,
      "step": 20600
    },
    {
      "epoch": 2.4754843338914134,
      "grad_norm": 4.673750877380371,
      "learning_rate": 1.0098062664434347e-05,
      "loss": 0.4621,
      "step": 20700
    },
    {
      "epoch": 2.4874431954077973,
      "grad_norm": 8.451375007629395,
      "learning_rate": 1.0050227218368814e-05,
      "loss": 0.4509,
      "step": 20800
    },
    {
      "epoch": 2.499402056924181,
      "grad_norm": 11.161206245422363,
      "learning_rate": 1.0002391772303277e-05,
      "loss": 0.3874,
      "step": 20900
    },
    {
      "epoch": 2.5113609184405643,
      "grad_norm": 7.594831943511963,
      "learning_rate": 9.954556326237742e-06,
      "loss": 0.5169,
      "step": 21000
    },
    {
      "epoch": 2.5233197799569482,
      "grad_norm": 11.392334938049316,
      "learning_rate": 9.906720880172208e-06,
      "loss": 0.4727,
      "step": 21100
    },
    {
      "epoch": 2.5352786414733317,
      "grad_norm": 6.808231830596924,
      "learning_rate": 9.858885434106675e-06,
      "loss": 0.4831,
      "step": 21200
    },
    {
      "epoch": 2.5472375029897156,
      "grad_norm": 25.36167335510254,
      "learning_rate": 9.81104998804114e-06,
      "loss": 0.4602,
      "step": 21300
    },
    {
      "epoch": 2.559196364506099,
      "grad_norm": 8.358457565307617,
      "learning_rate": 9.763214541975605e-06,
      "loss": 0.497,
      "step": 21400
    },
    {
      "epoch": 2.5711552260224826,
      "grad_norm": 1.5641921758651733,
      "learning_rate": 9.71537909591007e-06,
      "loss": 0.3724,
      "step": 21500
    },
    {
      "epoch": 2.583114087538866,
      "grad_norm": 3.978846549987793,
      "learning_rate": 9.667543649844535e-06,
      "loss": 0.4917,
      "step": 21600
    },
    {
      "epoch": 2.59507294905525,
      "grad_norm": 19.882680892944336,
      "learning_rate": 9.619708203779e-06,
      "loss": 0.493,
      "step": 21700
    },
    {
      "epoch": 2.6070318105716335,
      "grad_norm": 7.563614845275879,
      "learning_rate": 9.571872757713466e-06,
      "loss": 0.5144,
      "step": 21800
    },
    {
      "epoch": 2.6189906720880174,
      "grad_norm": 10.534144401550293,
      "learning_rate": 9.524037311647933e-06,
      "loss": 0.4108,
      "step": 21900
    },
    {
      "epoch": 2.630949533604401,
      "grad_norm": 8.63292407989502,
      "learning_rate": 9.476201865582398e-06,
      "loss": 0.4339,
      "step": 22000
    },
    {
      "epoch": 2.6429083951207843,
      "grad_norm": 3.7575886249542236,
      "learning_rate": 9.428366419516862e-06,
      "loss": 0.5199,
      "step": 22100
    },
    {
      "epoch": 2.6548672566371683,
      "grad_norm": 11.982815742492676,
      "learning_rate": 9.380530973451329e-06,
      "loss": 0.4705,
      "step": 22200
    },
    {
      "epoch": 2.6668261181535518,
      "grad_norm": 10.997756004333496,
      "learning_rate": 9.332695527385794e-06,
      "loss": 0.4588,
      "step": 22300
    },
    {
      "epoch": 2.6787849796699357,
      "grad_norm": 14.247201919555664,
      "learning_rate": 9.284860081320259e-06,
      "loss": 0.4754,
      "step": 22400
    },
    {
      "epoch": 2.690743841186319,
      "grad_norm": 9.389164924621582,
      "learning_rate": 9.237024635254724e-06,
      "loss": 0.4931,
      "step": 22500
    },
    {
      "epoch": 2.7027027027027026,
      "grad_norm": 5.165822982788086,
      "learning_rate": 9.189189189189191e-06,
      "loss": 0.5503,
      "step": 22600
    },
    {
      "epoch": 2.714661564219086,
      "grad_norm": 19.41639518737793,
      "learning_rate": 9.141353743123655e-06,
      "loss": 0.4392,
      "step": 22700
    },
    {
      "epoch": 2.72662042573547,
      "grad_norm": 13.325432777404785,
      "learning_rate": 9.09351829705812e-06,
      "loss": 0.466,
      "step": 22800
    },
    {
      "epoch": 2.7385792872518535,
      "grad_norm": 0.8282158374786377,
      "learning_rate": 9.045682850992587e-06,
      "loss": 0.5011,
      "step": 22900
    },
    {
      "epoch": 2.7505381487682374,
      "grad_norm": 1.6116673946380615,
      "learning_rate": 8.997847404927052e-06,
      "loss": 0.5046,
      "step": 23000
    },
    {
      "epoch": 2.762497010284621,
      "grad_norm": 7.066225051879883,
      "learning_rate": 8.950011958861517e-06,
      "loss": 0.501,
      "step": 23100
    },
    {
      "epoch": 2.7744558718010044,
      "grad_norm": 8.266989707946777,
      "learning_rate": 8.902176512795983e-06,
      "loss": 0.4536,
      "step": 23200
    },
    {
      "epoch": 2.7864147333173883,
      "grad_norm": 12.963321685791016,
      "learning_rate": 8.854341066730448e-06,
      "loss": 0.45,
      "step": 23300
    },
    {
      "epoch": 2.798373594833772,
      "grad_norm": 6.654133319854736,
      "learning_rate": 8.806505620664913e-06,
      "loss": 0.4325,
      "step": 23400
    },
    {
      "epoch": 2.8103324563501557,
      "grad_norm": 23.46738624572754,
      "learning_rate": 8.758670174599378e-06,
      "loss": 0.4,
      "step": 23500
    },
    {
      "epoch": 2.822291317866539,
      "grad_norm": 16.673980712890625,
      "learning_rate": 8.710834728533845e-06,
      "loss": 0.509,
      "step": 23600
    },
    {
      "epoch": 2.8342501793829227,
      "grad_norm": 2.975837469100952,
      "learning_rate": 8.662999282468309e-06,
      "loss": 0.5261,
      "step": 23700
    },
    {
      "epoch": 2.846209040899306,
      "grad_norm": 10.004451751708984,
      "learning_rate": 8.615163836402776e-06,
      "loss": 0.4979,
      "step": 23800
    },
    {
      "epoch": 2.85816790241569,
      "grad_norm": 13.37239933013916,
      "learning_rate": 8.567328390337241e-06,
      "loss": 0.4415,
      "step": 23900
    },
    {
      "epoch": 2.8701267639320736,
      "grad_norm": 41.52552795410156,
      "learning_rate": 8.519492944271706e-06,
      "loss": 0.4781,
      "step": 24000
    },
    {
      "epoch": 2.8820856254484575,
      "grad_norm": 16.376989364624023,
      "learning_rate": 8.471657498206172e-06,
      "loss": 0.4448,
      "step": 24100
    },
    {
      "epoch": 2.894044486964841,
      "grad_norm": 18.360980987548828,
      "learning_rate": 8.423822052140637e-06,
      "loss": 0.4415,
      "step": 24200
    },
    {
      "epoch": 2.9060033484812244,
      "grad_norm": 17.686561584472656,
      "learning_rate": 8.375986606075102e-06,
      "loss": 0.4521,
      "step": 24300
    },
    {
      "epoch": 2.9179622099976084,
      "grad_norm": 8.287282943725586,
      "learning_rate": 8.328151160009567e-06,
      "loss": 0.4762,
      "step": 24400
    },
    {
      "epoch": 2.929921071513992,
      "grad_norm": 13.17303466796875,
      "learning_rate": 8.280315713944032e-06,
      "loss": 0.518,
      "step": 24500
    },
    {
      "epoch": 2.9418799330303758,
      "grad_norm": 16.951007843017578,
      "learning_rate": 8.2324802678785e-06,
      "loss": 0.4111,
      "step": 24600
    },
    {
      "epoch": 2.9538387945467592,
      "grad_norm": 1.8427983522415161,
      "learning_rate": 8.184644821812963e-06,
      "loss": 0.5199,
      "step": 24700
    },
    {
      "epoch": 2.9657976560631427,
      "grad_norm": 0.8009694218635559,
      "learning_rate": 8.13680937574743e-06,
      "loss": 0.4651,
      "step": 24800
    },
    {
      "epoch": 2.977756517579526,
      "grad_norm": 11.94599437713623,
      "learning_rate": 8.088973929681895e-06,
      "loss": 0.4742,
      "step": 24900
    },
    {
      "epoch": 2.98971537909591,
      "grad_norm": 6.332881927490234,
      "learning_rate": 8.04113848361636e-06,
      "loss": 0.4741,
      "step": 25000
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.8997608013155928,
      "eval_f1_macro": 0.8946290015766484,
      "eval_loss": 0.3225875496864319,
      "eval_runtime": 133.3939,
      "eval_samples_per_second": 100.289,
      "eval_steps_per_second": 6.275,
      "step": 25086
    },
    {
      "epoch": 3.0016742406122936,
      "grad_norm": 15.073138236999512,
      "learning_rate": 7.993303037550826e-06,
      "loss": 0.3672,
      "step": 25100
    },
    {
      "epoch": 3.0136331021286775,
      "grad_norm": 38.399200439453125,
      "learning_rate": 7.945467591485291e-06,
      "loss": 0.3951,
      "step": 25200
    },
    {
      "epoch": 3.025591963645061,
      "grad_norm": 33.6447639465332,
      "learning_rate": 7.897632145419756e-06,
      "loss": 0.4142,
      "step": 25300
    },
    {
      "epoch": 3.0375508251614445,
      "grad_norm": 0.6571112275123596,
      "learning_rate": 7.849796699354221e-06,
      "loss": 0.3723,
      "step": 25400
    },
    {
      "epoch": 3.0495096866778284,
      "grad_norm": 15.047953605651855,
      "learning_rate": 7.801961253288688e-06,
      "loss": 0.3853,
      "step": 25500
    },
    {
      "epoch": 3.061468548194212,
      "grad_norm": 20.45806312561035,
      "learning_rate": 7.754125807223154e-06,
      "loss": 0.3423,
      "step": 25600
    },
    {
      "epoch": 3.0734274097105954,
      "grad_norm": 8.727627754211426,
      "learning_rate": 7.706290361157619e-06,
      "loss": 0.362,
      "step": 25700
    },
    {
      "epoch": 3.0853862712269793,
      "grad_norm": 13.270479202270508,
      "learning_rate": 7.658454915092084e-06,
      "loss": 0.3737,
      "step": 25800
    },
    {
      "epoch": 3.0973451327433628,
      "grad_norm": 13.634718894958496,
      "learning_rate": 7.610619469026549e-06,
      "loss": 0.4595,
      "step": 25900
    },
    {
      "epoch": 3.1093039942597467,
      "grad_norm": 15.934784889221191,
      "learning_rate": 7.562784022961015e-06,
      "loss": 0.3793,
      "step": 26000
    },
    {
      "epoch": 3.12126285577613,
      "grad_norm": 27.4837703704834,
      "learning_rate": 7.51494857689548e-06,
      "loss": 0.3569,
      "step": 26100
    },
    {
      "epoch": 3.1332217172925136,
      "grad_norm": 10.982233047485352,
      "learning_rate": 7.467113130829946e-06,
      "loss": 0.3347,
      "step": 26200
    },
    {
      "epoch": 3.1451805788088976,
      "grad_norm": 0.13693156838417053,
      "learning_rate": 7.419277684764411e-06,
      "loss": 0.3304,
      "step": 26300
    },
    {
      "epoch": 3.157139440325281,
      "grad_norm": 0.5687864422798157,
      "learning_rate": 7.371442238698876e-06,
      "loss": 0.3072,
      "step": 26400
    },
    {
      "epoch": 3.1690983018416645,
      "grad_norm": 3.3050904273986816,
      "learning_rate": 7.323606792633342e-06,
      "loss": 0.3919,
      "step": 26500
    },
    {
      "epoch": 3.1810571633580484,
      "grad_norm": 44.54555130004883,
      "learning_rate": 7.275771346567807e-06,
      "loss": 0.4095,
      "step": 26600
    },
    {
      "epoch": 3.193016024874432,
      "grad_norm": 6.933104038238525,
      "learning_rate": 7.227935900502273e-06,
      "loss": 0.3589,
      "step": 26700
    },
    {
      "epoch": 3.2049748863908154,
      "grad_norm": 6.798495769500732,
      "learning_rate": 7.180100454436738e-06,
      "loss": 0.3922,
      "step": 26800
    },
    {
      "epoch": 3.2169337479071993,
      "grad_norm": 1.6484425067901611,
      "learning_rate": 7.132265008371204e-06,
      "loss": 0.402,
      "step": 26900
    },
    {
      "epoch": 3.228892609423583,
      "grad_norm": 0.04557349905371666,
      "learning_rate": 7.084429562305669e-06,
      "loss": 0.4111,
      "step": 27000
    },
    {
      "epoch": 3.2408514709399663,
      "grad_norm": 18.44191551208496,
      "learning_rate": 7.036594116240134e-06,
      "loss": 0.3452,
      "step": 27100
    },
    {
      "epoch": 3.25281033245635,
      "grad_norm": 18.60299301147461,
      "learning_rate": 6.9887586701746e-06,
      "loss": 0.3824,
      "step": 27200
    },
    {
      "epoch": 3.2647691939727337,
      "grad_norm": 0.6905956268310547,
      "learning_rate": 6.940923224109065e-06,
      "loss": 0.4061,
      "step": 27300
    },
    {
      "epoch": 3.2767280554891176,
      "grad_norm": 33.460792541503906,
      "learning_rate": 6.893087778043531e-06,
      "loss": 0.3307,
      "step": 27400
    },
    {
      "epoch": 3.288686917005501,
      "grad_norm": 1.2814239263534546,
      "learning_rate": 6.8452523319779964e-06,
      "loss": 0.3659,
      "step": 27500
    },
    {
      "epoch": 3.3006457785218846,
      "grad_norm": 11.21789836883545,
      "learning_rate": 6.797416885912461e-06,
      "loss": 0.4138,
      "step": 27600
    },
    {
      "epoch": 3.3126046400382685,
      "grad_norm": 45.998199462890625,
      "learning_rate": 6.749581439846927e-06,
      "loss": 0.4118,
      "step": 27700
    },
    {
      "epoch": 3.324563501554652,
      "grad_norm": 6.786585330963135,
      "learning_rate": 6.701745993781392e-06,
      "loss": 0.3961,
      "step": 27800
    },
    {
      "epoch": 3.3365223630710354,
      "grad_norm": 0.49516820907592773,
      "learning_rate": 6.653910547715858e-06,
      "loss": 0.3556,
      "step": 27900
    },
    {
      "epoch": 3.3484812245874194,
      "grad_norm": 16.676923751831055,
      "learning_rate": 6.6060751016503235e-06,
      "loss": 0.3598,
      "step": 28000
    },
    {
      "epoch": 3.360440086103803,
      "grad_norm": 17.03455924987793,
      "learning_rate": 6.5582396555847895e-06,
      "loss": 0.3467,
      "step": 28100
    },
    {
      "epoch": 3.3723989476201863,
      "grad_norm": 4.704310894012451,
      "learning_rate": 6.510404209519254e-06,
      "loss": 0.3819,
      "step": 28200
    },
    {
      "epoch": 3.3843578091365702,
      "grad_norm": 17.95856285095215,
      "learning_rate": 6.462568763453719e-06,
      "loss": 0.3938,
      "step": 28300
    },
    {
      "epoch": 3.3963166706529537,
      "grad_norm": 6.905900955200195,
      "learning_rate": 6.414733317388185e-06,
      "loss": 0.3595,
      "step": 28400
    },
    {
      "epoch": 3.4082755321693377,
      "grad_norm": 11.372364044189453,
      "learning_rate": 6.3668978713226505e-06,
      "loss": 0.4367,
      "step": 28500
    },
    {
      "epoch": 3.420234393685721,
      "grad_norm": 3.1483750343322754,
      "learning_rate": 6.319062425257117e-06,
      "loss": 0.3793,
      "step": 28600
    },
    {
      "epoch": 3.4321932552021046,
      "grad_norm": 0.5533880591392517,
      "learning_rate": 6.271226979191582e-06,
      "loss": 0.3708,
      "step": 28700
    },
    {
      "epoch": 3.4441521167184885,
      "grad_norm": 9.670879364013672,
      "learning_rate": 6.223391533126046e-06,
      "loss": 0.3523,
      "step": 28800
    },
    {
      "epoch": 3.456110978234872,
      "grad_norm": 6.2539448738098145,
      "learning_rate": 6.175556087060512e-06,
      "loss": 0.3983,
      "step": 28900
    },
    {
      "epoch": 3.4680698397512555,
      "grad_norm": 8.685084342956543,
      "learning_rate": 6.1277206409949775e-06,
      "loss": 0.3469,
      "step": 29000
    },
    {
      "epoch": 3.4800287012676394,
      "grad_norm": 1.831606388092041,
      "learning_rate": 6.079885194929444e-06,
      "loss": 0.3873,
      "step": 29100
    },
    {
      "epoch": 3.491987562784023,
      "grad_norm": 7.324891090393066,
      "learning_rate": 6.032049748863909e-06,
      "loss": 0.3759,
      "step": 29200
    },
    {
      "epoch": 3.5039464243004064,
      "grad_norm": 4.640222549438477,
      "learning_rate": 5.984214302798375e-06,
      "loss": 0.35,
      "step": 29300
    },
    {
      "epoch": 3.5159052858167903,
      "grad_norm": 26.190948486328125,
      "learning_rate": 5.936378856732839e-06,
      "loss": 0.3581,
      "step": 29400
    },
    {
      "epoch": 3.5278641473331738,
      "grad_norm": 9.630280494689941,
      "learning_rate": 5.888543410667305e-06,
      "loss": 0.3572,
      "step": 29500
    },
    {
      "epoch": 3.5398230088495577,
      "grad_norm": 7.106590747833252,
      "learning_rate": 5.840707964601771e-06,
      "loss": 0.4053,
      "step": 29600
    },
    {
      "epoch": 3.551781870365941,
      "grad_norm": 8.433319091796875,
      "learning_rate": 5.792872518536236e-06,
      "loss": 0.3669,
      "step": 29700
    },
    {
      "epoch": 3.5637407318823247,
      "grad_norm": 0.45837634801864624,
      "learning_rate": 5.745037072470702e-06,
      "loss": 0.3496,
      "step": 29800
    },
    {
      "epoch": 3.5756995933987086,
      "grad_norm": 35.96790313720703,
      "learning_rate": 5.697201626405166e-06,
      "loss": 0.3846,
      "step": 29900
    },
    {
      "epoch": 3.587658454915092,
      "grad_norm": 9.078967094421387,
      "learning_rate": 5.649366180339632e-06,
      "loss": 0.3586,
      "step": 30000
    },
    {
      "epoch": 3.599617316431476,
      "grad_norm": 13.171775817871094,
      "learning_rate": 5.601530734274098e-06,
      "loss": 0.4058,
      "step": 30100
    },
    {
      "epoch": 3.6115761779478595,
      "grad_norm": 4.815081596374512,
      "learning_rate": 5.553695288208563e-06,
      "loss": 0.3376,
      "step": 30200
    },
    {
      "epoch": 3.623535039464243,
      "grad_norm": 45.747379302978516,
      "learning_rate": 5.505859842143029e-06,
      "loss": 0.3312,
      "step": 30300
    },
    {
      "epoch": 3.6354939009806264,
      "grad_norm": 14.268325805664062,
      "learning_rate": 5.458024396077493e-06,
      "loss": 0.361,
      "step": 30400
    },
    {
      "epoch": 3.6474527624970103,
      "grad_norm": 26.025379180908203,
      "learning_rate": 5.4101889500119595e-06,
      "loss": 0.4716,
      "step": 30500
    },
    {
      "epoch": 3.659411624013394,
      "grad_norm": 9.367374420166016,
      "learning_rate": 5.362353503946425e-06,
      "loss": 0.3993,
      "step": 30600
    },
    {
      "epoch": 3.6713704855297777,
      "grad_norm": 0.5937235355377197,
      "learning_rate": 5.31451805788089e-06,
      "loss": 0.4208,
      "step": 30700
    },
    {
      "epoch": 3.683329347046161,
      "grad_norm": 8.220746040344238,
      "learning_rate": 5.266682611815356e-06,
      "loss": 0.3465,
      "step": 30800
    },
    {
      "epoch": 3.6952882085625447,
      "grad_norm": 18.557844161987305,
      "learning_rate": 5.2188471657498205e-06,
      "loss": 0.4118,
      "step": 30900
    },
    {
      "epoch": 3.7072470700789286,
      "grad_norm": 1.3928014039993286,
      "learning_rate": 5.1710117196842865e-06,
      "loss": 0.3436,
      "step": 31000
    },
    {
      "epoch": 3.719205931595312,
      "grad_norm": 5.382943630218506,
      "learning_rate": 5.123176273618752e-06,
      "loss": 0.3543,
      "step": 31100
    },
    {
      "epoch": 3.731164793111696,
      "grad_norm": 14.795188903808594,
      "learning_rate": 5.075340827553218e-06,
      "loss": 0.3535,
      "step": 31200
    },
    {
      "epoch": 3.7431236546280795,
      "grad_norm": 6.450940132141113,
      "learning_rate": 5.027505381487683e-06,
      "loss": 0.3714,
      "step": 31300
    },
    {
      "epoch": 3.755082516144463,
      "grad_norm": 13.583833694458008,
      "learning_rate": 4.979669935422148e-06,
      "loss": 0.354,
      "step": 31400
    },
    {
      "epoch": 3.7670413776608465,
      "grad_norm": 7.967719554901123,
      "learning_rate": 4.931834489356614e-06,
      "loss": 0.328,
      "step": 31500
    },
    {
      "epoch": 3.7790002391772304,
      "grad_norm": 1.5704030990600586,
      "learning_rate": 4.883999043291079e-06,
      "loss": 0.3888,
      "step": 31600
    },
    {
      "epoch": 3.790959100693614,
      "grad_norm": 28.094955444335938,
      "learning_rate": 4.836163597225544e-06,
      "loss": 0.4169,
      "step": 31700
    },
    {
      "epoch": 3.802917962209998,
      "grad_norm": 13.572942733764648,
      "learning_rate": 4.78832815116001e-06,
      "loss": 0.4251,
      "step": 31800
    },
    {
      "epoch": 3.8148768237263813,
      "grad_norm": 26.280473709106445,
      "learning_rate": 4.740492705094475e-06,
      "loss": 0.4045,
      "step": 31900
    },
    {
      "epoch": 3.8268356852427647,
      "grad_norm": 18.53355598449707,
      "learning_rate": 4.692657259028941e-06,
      "loss": 0.3918,
      "step": 32000
    },
    {
      "epoch": 3.8387945467591487,
      "grad_norm": 6.290987014770508,
      "learning_rate": 4.644821812963407e-06,
      "loss": 0.3738,
      "step": 32100
    },
    {
      "epoch": 3.850753408275532,
      "grad_norm": 9.246485710144043,
      "learning_rate": 4.596986366897871e-06,
      "loss": 0.4073,
      "step": 32200
    },
    {
      "epoch": 3.862712269791916,
      "grad_norm": 31.59609603881836,
      "learning_rate": 4.549150920832337e-06,
      "loss": 0.3779,
      "step": 32300
    },
    {
      "epoch": 3.8746711313082995,
      "grad_norm": 18.260915756225586,
      "learning_rate": 4.5013154747668024e-06,
      "loss": 0.3941,
      "step": 32400
    },
    {
      "epoch": 3.886629992824683,
      "grad_norm": 17.855915069580078,
      "learning_rate": 4.453480028701268e-06,
      "loss": 0.3744,
      "step": 32500
    },
    {
      "epoch": 3.8985888543410665,
      "grad_norm": 20.609582901000977,
      "learning_rate": 4.405644582635734e-06,
      "loss": 0.3704,
      "step": 32600
    },
    {
      "epoch": 3.9105477158574504,
      "grad_norm": 3.4865665435791016,
      "learning_rate": 4.357809136570199e-06,
      "loss": 0.3888,
      "step": 32700
    },
    {
      "epoch": 3.922506577373834,
      "grad_norm": 24.210630416870117,
      "learning_rate": 4.309973690504664e-06,
      "loss": 0.3553,
      "step": 32800
    },
    {
      "epoch": 3.934465438890218,
      "grad_norm": 1.428337574005127,
      "learning_rate": 4.2621382444391295e-06,
      "loss": 0.3412,
      "step": 32900
    },
    {
      "epoch": 3.9464243004066013,
      "grad_norm": 3.867114543914795,
      "learning_rate": 4.2143027983735956e-06,
      "loss": 0.3711,
      "step": 33000
    },
    {
      "epoch": 3.958383161922985,
      "grad_norm": 19.748249053955078,
      "learning_rate": 4.166467352308061e-06,
      "loss": 0.4051,
      "step": 33100
    },
    {
      "epoch": 3.9703420234393687,
      "grad_norm": 7.413752555847168,
      "learning_rate": 4.118631906242526e-06,
      "loss": 0.3512,
      "step": 33200
    },
    {
      "epoch": 3.982300884955752,
      "grad_norm": 13.559946060180664,
      "learning_rate": 4.070796460176992e-06,
      "loss": 0.3628,
      "step": 33300
    },
    {
      "epoch": 3.994259746472136,
      "grad_norm": 21.954843521118164,
      "learning_rate": 4.0229610141114565e-06,
      "loss": 0.4173,
      "step": 33400
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.9308566302885334,
      "eval_f1_macro": 0.922591630220665,
      "eval_loss": 0.22896160185337067,
      "eval_runtime": 114.522,
      "eval_samples_per_second": 116.816,
      "eval_steps_per_second": 7.309,
      "step": 33448
    },
    {
      "epoch": 4.00621860798852,
      "grad_norm": 6.975641250610352,
      "learning_rate": 3.975125568045923e-06,
      "loss": 0.2991,
      "step": 33500
    },
    {
      "epoch": 4.018177469504903,
      "grad_norm": 9.624658584594727,
      "learning_rate": 3.927290121980388e-06,
      "loss": 0.3113,
      "step": 33600
    },
    {
      "epoch": 4.0301363310212865,
      "grad_norm": 0.9042490720748901,
      "learning_rate": 3.879454675914853e-06,
      "loss": 0.2856,
      "step": 33700
    },
    {
      "epoch": 4.04209519253767,
      "grad_norm": 0.16978619992733002,
      "learning_rate": 3.831619229849319e-06,
      "loss": 0.3049,
      "step": 33800
    },
    {
      "epoch": 4.054054054054054,
      "grad_norm": 9.018377304077148,
      "learning_rate": 3.7837837837837844e-06,
      "loss": 0.3519,
      "step": 33900
    },
    {
      "epoch": 4.066012915570438,
      "grad_norm": 2.0034077167510986,
      "learning_rate": 3.735948337718249e-06,
      "loss": 0.2898,
      "step": 34000
    },
    {
      "epoch": 4.077971777086821,
      "grad_norm": 19.824430465698242,
      "learning_rate": 3.688112891652715e-06,
      "loss": 0.3563,
      "step": 34100
    },
    {
      "epoch": 4.089930638603205,
      "grad_norm": 0.9186412692070007,
      "learning_rate": 3.6402774455871805e-06,
      "loss": 0.3426,
      "step": 34200
    },
    {
      "epoch": 4.101889500119588,
      "grad_norm": 6.762730121612549,
      "learning_rate": 3.5924419995216458e-06,
      "loss": 0.2995,
      "step": 34300
    },
    {
      "epoch": 4.113848361635973,
      "grad_norm": 10.401214599609375,
      "learning_rate": 3.5446065534561114e-06,
      "loss": 0.3434,
      "step": 34400
    },
    {
      "epoch": 4.125807223152356,
      "grad_norm": 30.29718780517578,
      "learning_rate": 3.496771107390577e-06,
      "loss": 0.273,
      "step": 34500
    },
    {
      "epoch": 4.13776608466874,
      "grad_norm": 33.84376907348633,
      "learning_rate": 3.448935661325042e-06,
      "loss": 0.2888,
      "step": 34600
    },
    {
      "epoch": 4.149724946185123,
      "grad_norm": 61.454368591308594,
      "learning_rate": 3.4011002152595076e-06,
      "loss": 0.2439,
      "step": 34700
    },
    {
      "epoch": 4.161683807701507,
      "grad_norm": 40.19211959838867,
      "learning_rate": 3.353264769193973e-06,
      "loss": 0.299,
      "step": 34800
    },
    {
      "epoch": 4.17364266921789,
      "grad_norm": 0.27534738183021545,
      "learning_rate": 3.3054293231284385e-06,
      "loss": 0.2218,
      "step": 34900
    },
    {
      "epoch": 4.185601530734274,
      "grad_norm": 40.53688049316406,
      "learning_rate": 3.257593877062904e-06,
      "loss": 0.3059,
      "step": 35000
    },
    {
      "epoch": 4.197560392250658,
      "grad_norm": 20.62453269958496,
      "learning_rate": 3.2097584309973694e-06,
      "loss": 0.3062,
      "step": 35100
    },
    {
      "epoch": 4.209519253767041,
      "grad_norm": 15.372516632080078,
      "learning_rate": 3.1619229849318346e-06,
      "loss": 0.3306,
      "step": 35200
    },
    {
      "epoch": 4.221478115283425,
      "grad_norm": 4.625638961791992,
      "learning_rate": 3.1140875388663e-06,
      "loss": 0.2861,
      "step": 35300
    },
    {
      "epoch": 4.233436976799808,
      "grad_norm": 3.5799901485443115,
      "learning_rate": 3.0662520928007655e-06,
      "loss": 0.2646,
      "step": 35400
    },
    {
      "epoch": 4.245395838316193,
      "grad_norm": 0.33352604508399963,
      "learning_rate": 3.018416646735231e-06,
      "loss": 0.325,
      "step": 35500
    },
    {
      "epoch": 4.257354699832576,
      "grad_norm": 19.27859115600586,
      "learning_rate": 2.9705812006696964e-06,
      "loss": 0.3284,
      "step": 35600
    },
    {
      "epoch": 4.26931356134896,
      "grad_norm": 12.599363327026367,
      "learning_rate": 2.922745754604162e-06,
      "loss": 0.3464,
      "step": 35700
    },
    {
      "epoch": 4.281272422865343,
      "grad_norm": 15.769466400146484,
      "learning_rate": 2.8749103085386277e-06,
      "loss": 0.3152,
      "step": 35800
    },
    {
      "epoch": 4.293231284381727,
      "grad_norm": 3.5566675662994385,
      "learning_rate": 2.8270748624730925e-06,
      "loss": 0.3302,
      "step": 35900
    },
    {
      "epoch": 4.30519014589811,
      "grad_norm": 16.479644775390625,
      "learning_rate": 2.779239416407558e-06,
      "loss": 0.3027,
      "step": 36000
    },
    {
      "epoch": 4.3171490074144945,
      "grad_norm": 42.525516510009766,
      "learning_rate": 2.7314039703420234e-06,
      "loss": 0.243,
      "step": 36100
    },
    {
      "epoch": 4.329107868930878,
      "grad_norm": 1.127282738685608,
      "learning_rate": 2.683568524276489e-06,
      "loss": 0.3568,
      "step": 36200
    },
    {
      "epoch": 4.341066730447261,
      "grad_norm": 13.89721965789795,
      "learning_rate": 2.6357330782109548e-06,
      "loss": 0.2951,
      "step": 36300
    },
    {
      "epoch": 4.353025591963645,
      "grad_norm": 1.8368905782699585,
      "learning_rate": 2.5878976321454204e-06,
      "loss": 0.2721,
      "step": 36400
    },
    {
      "epoch": 4.364984453480028,
      "grad_norm": 9.002046585083008,
      "learning_rate": 2.5400621860798852e-06,
      "loss": 0.2437,
      "step": 36500
    },
    {
      "epoch": 4.376943314996412,
      "grad_norm": 17.86702537536621,
      "learning_rate": 2.492226740014351e-06,
      "loss": 0.2814,
      "step": 36600
    },
    {
      "epoch": 4.388902176512796,
      "grad_norm": 5.502840518951416,
      "learning_rate": 2.444391293948816e-06,
      "loss": 0.316,
      "step": 36700
    },
    {
      "epoch": 4.40086103802918,
      "grad_norm": 29.47393798828125,
      "learning_rate": 2.396555847883282e-06,
      "loss": 0.3406,
      "step": 36800
    },
    {
      "epoch": 4.412819899545563,
      "grad_norm": 0.1484375,
      "learning_rate": 2.348720401817747e-06,
      "loss": 0.3033,
      "step": 36900
    },
    {
      "epoch": 4.424778761061947,
      "grad_norm": 16.635862350463867,
      "learning_rate": 2.3008849557522127e-06,
      "loss": 0.2965,
      "step": 37000
    },
    {
      "epoch": 4.43673762257833,
      "grad_norm": 25.65052604675293,
      "learning_rate": 2.253049509686678e-06,
      "loss": 0.261,
      "step": 37100
    },
    {
      "epoch": 4.4486964840947145,
      "grad_norm": 0.289169043302536,
      "learning_rate": 2.205214063621143e-06,
      "loss": 0.2908,
      "step": 37200
    },
    {
      "epoch": 4.460655345611098,
      "grad_norm": 19.432939529418945,
      "learning_rate": 2.157378617555609e-06,
      "loss": 0.3038,
      "step": 37300
    },
    {
      "epoch": 4.4726142071274815,
      "grad_norm": 16.158103942871094,
      "learning_rate": 2.1095431714900745e-06,
      "loss": 0.3284,
      "step": 37400
    },
    {
      "epoch": 4.484573068643865,
      "grad_norm": 33.62700653076172,
      "learning_rate": 2.0617077254245398e-06,
      "loss": 0.3485,
      "step": 37500
    },
    {
      "epoch": 4.496531930160248,
      "grad_norm": 9.718842506408691,
      "learning_rate": 2.013872279359005e-06,
      "loss": 0.3104,
      "step": 37600
    },
    {
      "epoch": 4.508490791676632,
      "grad_norm": 4.208487033843994,
      "learning_rate": 1.9660368332934707e-06,
      "loss": 0.2772,
      "step": 37700
    },
    {
      "epoch": 4.520449653193016,
      "grad_norm": 25.431921005249023,
      "learning_rate": 1.9182013872279363e-06,
      "loss": 0.3119,
      "step": 37800
    },
    {
      "epoch": 4.5324085147094,
      "grad_norm": 7.879194736480713,
      "learning_rate": 1.8703659411624013e-06,
      "loss": 0.3089,
      "step": 37900
    },
    {
      "epoch": 4.544367376225783,
      "grad_norm": 27.196775436401367,
      "learning_rate": 1.822530495096867e-06,
      "loss": 0.3258,
      "step": 38000
    },
    {
      "epoch": 4.556326237742167,
      "grad_norm": 2.055392265319824,
      "learning_rate": 1.7746950490313325e-06,
      "loss": 0.2861,
      "step": 38100
    },
    {
      "epoch": 4.568285099258551,
      "grad_norm": 20.772104263305664,
      "learning_rate": 1.7268596029657977e-06,
      "loss": 0.2719,
      "step": 38200
    },
    {
      "epoch": 4.580243960774935,
      "grad_norm": 42.49705123901367,
      "learning_rate": 1.6790241569002634e-06,
      "loss": 0.2949,
      "step": 38300
    },
    {
      "epoch": 4.592202822291318,
      "grad_norm": 1.5628721714019775,
      "learning_rate": 1.6311887108347288e-06,
      "loss": 0.3162,
      "step": 38400
    },
    {
      "epoch": 4.6041616838077015,
      "grad_norm": 51.489585876464844,
      "learning_rate": 1.583353264769194e-06,
      "loss": 0.3069,
      "step": 38500
    },
    {
      "epoch": 4.616120545324085,
      "grad_norm": 14.156673431396484,
      "learning_rate": 1.5355178187036595e-06,
      "loss": 0.3205,
      "step": 38600
    },
    {
      "epoch": 4.6280794068404685,
      "grad_norm": 0.20722483098506927,
      "learning_rate": 1.4876823726381252e-06,
      "loss": 0.2771,
      "step": 38700
    },
    {
      "epoch": 4.640038268356852,
      "grad_norm": 17.297119140625,
      "learning_rate": 1.4398469265725904e-06,
      "loss": 0.2736,
      "step": 38800
    },
    {
      "epoch": 4.651997129873236,
      "grad_norm": 1.102387547492981,
      "learning_rate": 1.3920114805070558e-06,
      "loss": 0.3052,
      "step": 38900
    },
    {
      "epoch": 4.66395599138962,
      "grad_norm": 22.904932022094727,
      "learning_rate": 1.3441760344415213e-06,
      "loss": 0.2392,
      "step": 39000
    },
    {
      "epoch": 4.675914852906003,
      "grad_norm": 24.05603790283203,
      "learning_rate": 1.2963405883759867e-06,
      "loss": 0.321,
      "step": 39100
    },
    {
      "epoch": 4.687873714422387,
      "grad_norm": 0.3016103208065033,
      "learning_rate": 1.2485051423104522e-06,
      "loss": 0.2886,
      "step": 39200
    },
    {
      "epoch": 4.69983257593877,
      "grad_norm": 12.00894832611084,
      "learning_rate": 1.2006696962449176e-06,
      "loss": 0.2719,
      "step": 39300
    },
    {
      "epoch": 4.711791437455155,
      "grad_norm": 18.27471923828125,
      "learning_rate": 1.152834250179383e-06,
      "loss": 0.3154,
      "step": 39400
    },
    {
      "epoch": 4.723750298971538,
      "grad_norm": 3.7251741886138916,
      "learning_rate": 1.1049988041138485e-06,
      "loss": 0.2739,
      "step": 39500
    },
    {
      "epoch": 4.735709160487922,
      "grad_norm": 4.41551399230957,
      "learning_rate": 1.0571633580483138e-06,
      "loss": 0.3079,
      "step": 39600
    },
    {
      "epoch": 4.747668022004305,
      "grad_norm": 22.375024795532227,
      "learning_rate": 1.0093279119827794e-06,
      "loss": 0.3651,
      "step": 39700
    },
    {
      "epoch": 4.7596268835206885,
      "grad_norm": 11.75764274597168,
      "learning_rate": 9.614924659172447e-07,
      "loss": 0.307,
      "step": 39800
    },
    {
      "epoch": 4.771585745037072,
      "grad_norm": 5.3026580810546875,
      "learning_rate": 9.136570198517101e-07,
      "loss": 0.2933,
      "step": 39900
    },
    {
      "epoch": 4.783544606553456,
      "grad_norm": 21.245481491088867,
      "learning_rate": 8.658215737861757e-07,
      "loss": 0.3544,
      "step": 40000
    },
    {
      "epoch": 4.79550346806984,
      "grad_norm": 26.337553024291992,
      "learning_rate": 8.17986127720641e-07,
      "loss": 0.3225,
      "step": 40100
    },
    {
      "epoch": 4.807462329586223,
      "grad_norm": 19.78595733642578,
      "learning_rate": 7.701506816551065e-07,
      "loss": 0.3123,
      "step": 40200
    },
    {
      "epoch": 4.819421191102607,
      "grad_norm": 19.528379440307617,
      "learning_rate": 7.223152355895719e-07,
      "loss": 0.2881,
      "step": 40300
    },
    {
      "epoch": 4.83138005261899,
      "grad_norm": 7.447572231292725,
      "learning_rate": 6.744797895240374e-07,
      "loss": 0.3259,
      "step": 40400
    },
    {
      "epoch": 4.843338914135375,
      "grad_norm": 16.28931999206543,
      "learning_rate": 6.266443434585027e-07,
      "loss": 0.2915,
      "step": 40500
    },
    {
      "epoch": 4.855297775651758,
      "grad_norm": 0.02465413510799408,
      "learning_rate": 5.788088973929682e-07,
      "loss": 0.3013,
      "step": 40600
    },
    {
      "epoch": 4.867256637168142,
      "grad_norm": 0.06680174171924591,
      "learning_rate": 5.309734513274336e-07,
      "loss": 0.2778,
      "step": 40700
    },
    {
      "epoch": 4.879215498684525,
      "grad_norm": 6.898055076599121,
      "learning_rate": 4.831380052618991e-07,
      "loss": 0.2674,
      "step": 40800
    },
    {
      "epoch": 4.891174360200909,
      "grad_norm": 33.961063385009766,
      "learning_rate": 4.3530255919636453e-07,
      "loss": 0.2091,
      "step": 40900
    },
    {
      "epoch": 4.903133221717292,
      "grad_norm": 12.219071388244629,
      "learning_rate": 3.8746711313083e-07,
      "loss": 0.265,
      "step": 41000
    },
    {
      "epoch": 4.915092083233676,
      "grad_norm": 34.7811164855957,
      "learning_rate": 3.3963166706529543e-07,
      "loss": 0.323,
      "step": 41100
    },
    {
      "epoch": 4.92705094475006,
      "grad_norm": 45.07106018066406,
      "learning_rate": 2.9179622099976083e-07,
      "loss": 0.2984,
      "step": 41200
    },
    {
      "epoch": 4.939009806266443,
      "grad_norm": 1.557522177696228,
      "learning_rate": 2.439607749342263e-07,
      "loss": 0.3107,
      "step": 41300
    },
    {
      "epoch": 4.950968667782827,
      "grad_norm": 6.1565775871276855,
      "learning_rate": 1.9612532886869173e-07,
      "loss": 0.3073,
      "step": 41400
    },
    {
      "epoch": 4.96292752929921,
      "grad_norm": 0.2271164357662201,
      "learning_rate": 1.4828988280315715e-07,
      "loss": 0.2505,
      "step": 41500
    },
    {
      "epoch": 4.974886390815595,
      "grad_norm": 0.9415814280509949,
      "learning_rate": 1.0045443673762258e-07,
      "loss": 0.2681,
      "step": 41600
    },
    {
      "epoch": 4.986845252331978,
      "grad_norm": 31.127452850341797,
      "learning_rate": 5.261899067208802e-08,
      "loss": 0.2787,
      "step": 41700
    },
    {
      "epoch": 4.998804113848362,
      "grad_norm": 0.056742336601018906,
      "learning_rate": 4.783544606553457e-09,
      "loss": 0.292,
      "step": 41800
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.9454328001195993,
      "eval_f1_macro": 0.937441472041554,
      "eval_loss": 0.20569337904453278,
      "eval_runtime": 114.0438,
      "eval_samples_per_second": 117.306,
      "eval_steps_per_second": 7.339,
      "step": 41810
    }
  ],
  "logging_steps": 100,
  "max_steps": 41810,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 1.10002788717696e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
