{
  "best_metric": 0.812039124462268,
  "best_model_checkpoint": "./deberta_v3_emotion_hashtags\\checkpoint-8362",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 8362,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01195886151638364,
      "grad_norm": 10.08096981048584,
      "learning_rate": 1.995216455393447e-05,
      "loss": 1.5546,
      "step": 100
    },
    {
      "epoch": 0.02391772303276728,
      "grad_norm": 14.042702674865723,
      "learning_rate": 1.9904329107868932e-05,
      "loss": 1.2617,
      "step": 200
    },
    {
      "epoch": 0.03587658454915092,
      "grad_norm": 9.697357177734375,
      "learning_rate": 1.98564936618034e-05,
      "loss": 1.0324,
      "step": 300
    },
    {
      "epoch": 0.04783544606553456,
      "grad_norm": 10.099104881286621,
      "learning_rate": 1.9808658215737863e-05,
      "loss": 0.8648,
      "step": 400
    },
    {
      "epoch": 0.0597943075819182,
      "grad_norm": 15.621537208557129,
      "learning_rate": 1.976082276967233e-05,
      "loss": 0.8629,
      "step": 500
    },
    {
      "epoch": 0.07175316909830184,
      "grad_norm": 7.644753932952881,
      "learning_rate": 1.9712987323606793e-05,
      "loss": 0.8331,
      "step": 600
    },
    {
      "epoch": 0.08371203061468548,
      "grad_norm": 13.018353462219238,
      "learning_rate": 1.966515187754126e-05,
      "loss": 0.8118,
      "step": 700
    },
    {
      "epoch": 0.09567089213106912,
      "grad_norm": 34.868568420410156,
      "learning_rate": 1.9617316431475727e-05,
      "loss": 0.7464,
      "step": 800
    },
    {
      "epoch": 0.10762975364745277,
      "grad_norm": 19.370105743408203,
      "learning_rate": 1.956948098541019e-05,
      "loss": 0.7588,
      "step": 900
    },
    {
      "epoch": 0.1195886151638364,
      "grad_norm": 20.716096878051758,
      "learning_rate": 1.9521645539344657e-05,
      "loss": 0.708,
      "step": 1000
    },
    {
      "epoch": 0.13154747668022004,
      "grad_norm": 9.95832347869873,
      "learning_rate": 1.947381009327912e-05,
      "loss": 0.7817,
      "step": 1100
    },
    {
      "epoch": 0.1435063381966037,
      "grad_norm": 7.512314319610596,
      "learning_rate": 1.9425974647213588e-05,
      "loss": 0.7353,
      "step": 1200
    },
    {
      "epoch": 0.15546519971298733,
      "grad_norm": 8.9541597366333,
      "learning_rate": 1.937813920114805e-05,
      "loss": 0.7289,
      "step": 1300
    },
    {
      "epoch": 0.16742406122937095,
      "grad_norm": 6.535177230834961,
      "learning_rate": 1.933030375508252e-05,
      "loss": 0.7066,
      "step": 1400
    },
    {
      "epoch": 0.1793829227457546,
      "grad_norm": 2.1675820350646973,
      "learning_rate": 1.9282468309016985e-05,
      "loss": 0.6482,
      "step": 1500
    },
    {
      "epoch": 0.19134178426213824,
      "grad_norm": 10.124380111694336,
      "learning_rate": 1.923463286295145e-05,
      "loss": 0.7834,
      "step": 1600
    },
    {
      "epoch": 0.2033006457785219,
      "grad_norm": 11.924543380737305,
      "learning_rate": 1.9186797416885912e-05,
      "loss": 0.7899,
      "step": 1700
    },
    {
      "epoch": 0.21525950729490553,
      "grad_norm": 8.078987121582031,
      "learning_rate": 1.913896197082038e-05,
      "loss": 0.7553,
      "step": 1800
    },
    {
      "epoch": 0.22721836881128918,
      "grad_norm": 8.449869155883789,
      "learning_rate": 1.9091126524754843e-05,
      "loss": 0.7453,
      "step": 1900
    },
    {
      "epoch": 0.2391772303276728,
      "grad_norm": 5.966902732849121,
      "learning_rate": 1.904329107868931e-05,
      "loss": 0.8052,
      "step": 2000
    },
    {
      "epoch": 0.25113609184405644,
      "grad_norm": 14.897256851196289,
      "learning_rate": 1.8995455632623777e-05,
      "loss": 0.6477,
      "step": 2100
    },
    {
      "epoch": 0.2630949533604401,
      "grad_norm": 7.84080696105957,
      "learning_rate": 1.8947620186558244e-05,
      "loss": 0.6636,
      "step": 2200
    },
    {
      "epoch": 0.27505381487682373,
      "grad_norm": 192.8009796142578,
      "learning_rate": 1.8899784740492707e-05,
      "loss": 0.7251,
      "step": 2300
    },
    {
      "epoch": 0.2870126763932074,
      "grad_norm": 10.211992263793945,
      "learning_rate": 1.885194929442717e-05,
      "loss": 0.7107,
      "step": 2400
    },
    {
      "epoch": 0.298971537909591,
      "grad_norm": 9.302628517150879,
      "learning_rate": 1.8804113848361638e-05,
      "loss": 0.7254,
      "step": 2500
    },
    {
      "epoch": 0.31093039942597467,
      "grad_norm": 7.219296455383301,
      "learning_rate": 1.87562784022961e-05,
      "loss": 0.6874,
      "step": 2600
    },
    {
      "epoch": 0.3228892609423583,
      "grad_norm": 6.387142658233643,
      "learning_rate": 1.8708442956230568e-05,
      "loss": 0.6996,
      "step": 2700
    },
    {
      "epoch": 0.3348481224587419,
      "grad_norm": 7.456765174865723,
      "learning_rate": 1.8660607510165035e-05,
      "loss": 0.6808,
      "step": 2800
    },
    {
      "epoch": 0.34680698397512555,
      "grad_norm": 8.012441635131836,
      "learning_rate": 1.86127720640995e-05,
      "loss": 0.7404,
      "step": 2900
    },
    {
      "epoch": 0.3587658454915092,
      "grad_norm": 13.470535278320312,
      "learning_rate": 1.8564936618033966e-05,
      "loss": 0.6565,
      "step": 3000
    },
    {
      "epoch": 0.37072470700789284,
      "grad_norm": 3.2570741176605225,
      "learning_rate": 1.851710117196843e-05,
      "loss": 0.628,
      "step": 3100
    },
    {
      "epoch": 0.3826835685242765,
      "grad_norm": 10.644498825073242,
      "learning_rate": 1.8469265725902896e-05,
      "loss": 0.6526,
      "step": 3200
    },
    {
      "epoch": 0.39464243004066013,
      "grad_norm": 7.0747480392456055,
      "learning_rate": 1.842143027983736e-05,
      "loss": 0.6916,
      "step": 3300
    },
    {
      "epoch": 0.4066012915570438,
      "grad_norm": 10.368650436401367,
      "learning_rate": 1.8373594833771827e-05,
      "loss": 0.7668,
      "step": 3400
    },
    {
      "epoch": 0.4185601530734274,
      "grad_norm": 8.98692798614502,
      "learning_rate": 1.8325759387706293e-05,
      "loss": 0.6355,
      "step": 3500
    },
    {
      "epoch": 0.43051901458981107,
      "grad_norm": 10.5388765335083,
      "learning_rate": 1.8277923941640757e-05,
      "loss": 0.6845,
      "step": 3600
    },
    {
      "epoch": 0.4424778761061947,
      "grad_norm": 12.547924995422363,
      "learning_rate": 1.823008849557522e-05,
      "loss": 0.6306,
      "step": 3700
    },
    {
      "epoch": 0.45443673762257836,
      "grad_norm": 17.434024810791016,
      "learning_rate": 1.8182253049509687e-05,
      "loss": 0.664,
      "step": 3800
    },
    {
      "epoch": 0.46639559913896195,
      "grad_norm": 10.674860000610352,
      "learning_rate": 1.8134417603444154e-05,
      "loss": 0.7647,
      "step": 3900
    },
    {
      "epoch": 0.4783544606553456,
      "grad_norm": 4.601812839508057,
      "learning_rate": 1.8086582157378618e-05,
      "loss": 0.7262,
      "step": 4000
    },
    {
      "epoch": 0.49031332217172924,
      "grad_norm": 7.633907318115234,
      "learning_rate": 1.8038746711313085e-05,
      "loss": 0.6705,
      "step": 4100
    },
    {
      "epoch": 0.5022721836881129,
      "grad_norm": 5.027950763702393,
      "learning_rate": 1.7990911265247552e-05,
      "loss": 0.6929,
      "step": 4200
    },
    {
      "epoch": 0.5142310452044966,
      "grad_norm": 7.682476997375488,
      "learning_rate": 1.7943075819182015e-05,
      "loss": 0.5985,
      "step": 4300
    },
    {
      "epoch": 0.5261899067208802,
      "grad_norm": 6.129551887512207,
      "learning_rate": 1.789524037311648e-05,
      "loss": 0.6506,
      "step": 4400
    },
    {
      "epoch": 0.5381487682372638,
      "grad_norm": 6.67578649520874,
      "learning_rate": 1.7847404927050946e-05,
      "loss": 0.6636,
      "step": 4500
    },
    {
      "epoch": 0.5501076297536475,
      "grad_norm": 4.884012222290039,
      "learning_rate": 1.7799569480985413e-05,
      "loss": 0.7869,
      "step": 4600
    },
    {
      "epoch": 0.562066491270031,
      "grad_norm": 15.302020072937012,
      "learning_rate": 1.7751734034919876e-05,
      "loss": 0.6876,
      "step": 4700
    },
    {
      "epoch": 0.5740253527864148,
      "grad_norm": 3.3374056816101074,
      "learning_rate": 1.7703898588854343e-05,
      "loss": 0.6504,
      "step": 4800
    },
    {
      "epoch": 0.5859842143027983,
      "grad_norm": 12.448984146118164,
      "learning_rate": 1.765606314278881e-05,
      "loss": 0.673,
      "step": 4900
    },
    {
      "epoch": 0.597943075819182,
      "grad_norm": 6.910583972930908,
      "learning_rate": 1.7608227696723274e-05,
      "loss": 0.6487,
      "step": 5000
    },
    {
      "epoch": 0.6099019373355656,
      "grad_norm": 5.234573841094971,
      "learning_rate": 1.7560392250657737e-05,
      "loss": 0.6494,
      "step": 5100
    },
    {
      "epoch": 0.6218607988519493,
      "grad_norm": 7.9048333168029785,
      "learning_rate": 1.7512556804592204e-05,
      "loss": 0.7375,
      "step": 5200
    },
    {
      "epoch": 0.6338196603683329,
      "grad_norm": 3.8897156715393066,
      "learning_rate": 1.746472135852667e-05,
      "loss": 0.6996,
      "step": 5300
    },
    {
      "epoch": 0.6457785218847166,
      "grad_norm": 0.877735435962677,
      "learning_rate": 1.7416885912461135e-05,
      "loss": 0.655,
      "step": 5400
    },
    {
      "epoch": 0.6577373834011002,
      "grad_norm": 11.931236267089844,
      "learning_rate": 1.73690504663956e-05,
      "loss": 0.6442,
      "step": 5500
    },
    {
      "epoch": 0.6696962449174838,
      "grad_norm": 8.28175163269043,
      "learning_rate": 1.7321215020330065e-05,
      "loss": 0.6795,
      "step": 5600
    },
    {
      "epoch": 0.6816551064338675,
      "grad_norm": 5.503127574920654,
      "learning_rate": 1.7273379574264532e-05,
      "loss": 0.7165,
      "step": 5700
    },
    {
      "epoch": 0.6936139679502511,
      "grad_norm": 11.059713363647461,
      "learning_rate": 1.7225544128198996e-05,
      "loss": 0.7041,
      "step": 5800
    },
    {
      "epoch": 0.7055728294666348,
      "grad_norm": 5.214011192321777,
      "learning_rate": 1.7177708682133463e-05,
      "loss": 0.7242,
      "step": 5900
    },
    {
      "epoch": 0.7175316909830184,
      "grad_norm": 4.18586540222168,
      "learning_rate": 1.712987323606793e-05,
      "loss": 0.7379,
      "step": 6000
    },
    {
      "epoch": 0.7294905524994021,
      "grad_norm": 16.325096130371094,
      "learning_rate": 1.7082037790002393e-05,
      "loss": 0.6776,
      "step": 6100
    },
    {
      "epoch": 0.7414494140157857,
      "grad_norm": 7.303318977355957,
      "learning_rate": 1.703420234393686e-05,
      "loss": 0.6482,
      "step": 6200
    },
    {
      "epoch": 0.7534082755321694,
      "grad_norm": 5.563650608062744,
      "learning_rate": 1.6986366897871324e-05,
      "loss": 0.6782,
      "step": 6300
    },
    {
      "epoch": 0.765367137048553,
      "grad_norm": 9.927305221557617,
      "learning_rate": 1.6938531451805787e-05,
      "loss": 0.6639,
      "step": 6400
    },
    {
      "epoch": 0.7773259985649367,
      "grad_norm": 5.329163074493408,
      "learning_rate": 1.6890696005740254e-05,
      "loss": 0.6943,
      "step": 6500
    },
    {
      "epoch": 0.7892848600813203,
      "grad_norm": 3.4567723274230957,
      "learning_rate": 1.684286055967472e-05,
      "loss": 0.682,
      "step": 6600
    },
    {
      "epoch": 0.8012437215977039,
      "grad_norm": 12.245083808898926,
      "learning_rate": 1.6795025113609188e-05,
      "loss": 0.6462,
      "step": 6700
    },
    {
      "epoch": 0.8132025831140876,
      "grad_norm": 6.390713214874268,
      "learning_rate": 1.674718966754365e-05,
      "loss": 0.6404,
      "step": 6800
    },
    {
      "epoch": 0.8251614446304711,
      "grad_norm": 15.113524436950684,
      "learning_rate": 1.669935422147812e-05,
      "loss": 0.6438,
      "step": 6900
    },
    {
      "epoch": 0.8371203061468548,
      "grad_norm": 5.212280750274658,
      "learning_rate": 1.6651518775412582e-05,
      "loss": 0.6993,
      "step": 7000
    },
    {
      "epoch": 0.8490791676632384,
      "grad_norm": 6.735912322998047,
      "learning_rate": 1.6603683329347045e-05,
      "loss": 0.6542,
      "step": 7100
    },
    {
      "epoch": 0.8610380291796221,
      "grad_norm": 8.5941801071167,
      "learning_rate": 1.6555847883281512e-05,
      "loss": 0.6963,
      "step": 7200
    },
    {
      "epoch": 0.8729968906960057,
      "grad_norm": 5.308830738067627,
      "learning_rate": 1.650801243721598e-05,
      "loss": 0.679,
      "step": 7300
    },
    {
      "epoch": 0.8849557522123894,
      "grad_norm": 7.844737529754639,
      "learning_rate": 1.6460176991150443e-05,
      "loss": 0.6558,
      "step": 7400
    },
    {
      "epoch": 0.896914613728773,
      "grad_norm": 12.089272499084473,
      "learning_rate": 1.641234154508491e-05,
      "loss": 0.7164,
      "step": 7500
    },
    {
      "epoch": 0.9088734752451567,
      "grad_norm": 9.289371490478516,
      "learning_rate": 1.6364506099019373e-05,
      "loss": 0.6176,
      "step": 7600
    },
    {
      "epoch": 0.9208323367615403,
      "grad_norm": 4.625111103057861,
      "learning_rate": 1.631667065295384e-05,
      "loss": 0.5448,
      "step": 7700
    },
    {
      "epoch": 0.9327911982779239,
      "grad_norm": 4.052725791931152,
      "learning_rate": 1.6268835206888304e-05,
      "loss": 0.683,
      "step": 7800
    },
    {
      "epoch": 0.9447500597943076,
      "grad_norm": 2.4190585613250732,
      "learning_rate": 1.622099976082277e-05,
      "loss": 0.5898,
      "step": 7900
    },
    {
      "epoch": 0.9567089213106912,
      "grad_norm": 6.39670467376709,
      "learning_rate": 1.6173164314757238e-05,
      "loss": 0.6589,
      "step": 8000
    },
    {
      "epoch": 0.9686677828270749,
      "grad_norm": 11.293486595153809,
      "learning_rate": 1.61253288686917e-05,
      "loss": 0.7016,
      "step": 8100
    },
    {
      "epoch": 0.9806266443434585,
      "grad_norm": 5.604856014251709,
      "learning_rate": 1.6077493422626168e-05,
      "loss": 0.5861,
      "step": 8200
    },
    {
      "epoch": 0.9925855058598422,
      "grad_norm": 6.713145732879639,
      "learning_rate": 1.602965797656063e-05,
      "loss": 0.6557,
      "step": 8300
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8129017790402153,
      "eval_f1_macro": 0.812039124462268,
      "eval_loss": 0.5573570728302002,
      "eval_runtime": 109.3535,
      "eval_samples_per_second": 122.337,
      "eval_steps_per_second": 7.654,
      "step": 8362
    }
  ],
  "logging_steps": 100,
  "max_steps": 41810,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 2200055774353920.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
